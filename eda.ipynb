{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9b227f",
   "metadata": {},
   "source": [
    "# EDA - HDB Resale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4c859",
   "metadata": {},
   "source": [
    "### Import Environment Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Python Modules\n",
    "\n",
    "## System Modules\n",
    "import sys\n",
    "from typing import Union, Optional\n",
    "import warnings\n",
    "\n",
    "## Essential Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import median_abs_deviation\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "\n",
    "## Graphical Modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "## Pipeline and Train Test Split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "## SciKit Learning Preprocessing  \n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "### SciKit Learn ML Models\n",
    "\n",
    "## Linear Models\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, QuantileRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "## Tree Based Linear Models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "## Performance Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "### Hyper-parameter Tuning\n",
    "\n",
    "## GridSearch CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "# now you can import normally from model_selection\n",
    "from sklearn.model_selection import HalvingGridSearchCV # type: ignore\n",
    "\n",
    "## explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "# now you can import normally from model_selection\n",
    "from sklearn.model_selection import HalvingRandomSearchCV # type: ignore\n",
    "\n",
    "## Optuna for Hyperparameter Tuning\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from optuna.distributions import IntDistribution, FloatDistribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a temporary placeholder. In a real end to end project, we need to connect to BigQuery and download the data from BigQuery using SQL extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09caa12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./data/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give an indication on how large the dataset is\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b123b9",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f272d",
   "metadata": {},
   "source": [
    "**We work for a property consultancy company. This company want to develop an end to end machine learning pipeline that could deliver housing price prediction to customer.**\n",
    "\n",
    "**Our task is to develop a machine learning model that could accurately predict the resale prices of HDB resale flats. This model will assist buyers or sellers in planning their budgets more effectively and set realistic expectations. This model also need to help buyers determine the type of flat they can afford and in which location. This model also should provide sellers with valuable information regarding the potential market value of their property.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561e2e4",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek on the first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any duplicates in the dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 304 duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the duplicates for visual inspection\n",
    "df[df.duplicated(keep=False)].sort_values(by=list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There could be two possible reasons for duplications. The first reason is duplicate submission to the system. The second reason is there is a very slim possibility that there could be a similar sales at the same location, same type, within the same 3 storey range and sell at a same price.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the percentage of duplicates in the dataset\n",
    "duplicates_percentage = df.duplicated().sum() / len(df) * 100\n",
    "print(f'The number of duplicates is {df.duplicated().sum()}, which is {duplicates_percentage:.2f}% of the entire dataset. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since duplicated data is less than 1% of the entire dataset, we will remove the duplicates as it would not have impacted our analysis or prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any duplicates in the dataset for confirmation\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf78aab",
   "metadata": {},
   "source": [
    "#### Detecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9cb9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any missing values in the dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the information and data structure of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns names and categorical columns names\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Print the numerical and categorical columns\n",
    "print(\"Numerical columns:\", numerical_columns)\n",
    "print(\"Categorical columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Anomaly Analysis for Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly analysis for numerical columns\n",
    "df[numerical_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are no anaomaly, the minimum and maximum falls within expected range. For resale flat that has commencement date on 2022, there are some exception to the MOP rules especially for SERS holders.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hdb.gov.sg/residential/living-in-an-hdb-flat/sers-and-upgrading-programmes/sers/sers-flat-owners/rehousing-options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure Anomaly Analysis for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the month column\n",
    "df['month'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can convert `month` to datetime format and convert it to year and month column. Then we set year as numeric and month as categorical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the town column\n",
    "df['town'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use one-hot encoding for town.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the flat_type column\n",
    "df['flat_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use ordinal encoding or one-hot encoding for flat_type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the block column\n",
    "df['block'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use block number for future enhancement, for now we will remove them first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the street_name column\n",
    "df['street_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use street name for future enhancement, for now we will remove them first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the storey_range column\n",
    "df['storey_range'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can convert them to numerical number by taking the average between the 2 numbers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the flat_model column\n",
    "df['flat_model'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use one-hot encoding for `flat_model`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique values of the remaining_lease column\n",
    "df['remaining_lease'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can convert remaining lease from years and months into numerical months only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are no data cleaning or imputation required before target and feature analysis. However, we will drop location features as these are not in our scope of analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns `block` and `street_name`\n",
    "df = df.drop(columns=['block', 'street_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numerical columns names\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical columns names\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bbb35",
   "metadata": {},
   "source": [
    "## Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resale price Date Range\n",
    "end = df['month'].max() \n",
    "start = df['month'].min()\n",
    "print(f'The resale price data starts from {start} and end on {end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resale price distribution\n",
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.histplot(data=df, x='resale_price', kde=True, ax=ax)\n",
    "\n",
    "ax.set_title('Distribution of Resale Price')\n",
    "ax.set_xlabel('Resale Price')\n",
    "ax.set_ylabel('Frequency of Flats Sold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the statistical summary of the resale price\n",
    "df['resale_price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute interquartile range of resale price\n",
    "resale_price_iqr = df['resale_price'].quantile(0.75) - df['resale_price'].quantile(0.25)\n",
    "resale_price_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot the distribution of the resale price\n",
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.boxplot(data=df, x='resale_price', vert=False)\n",
    "\n",
    "ax.set_title('Boxplot of Resale Price')\n",
    "ax.set_xlabel('Resale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The mean resale price is approximately $516K, while the median is about $485K. Transaction prices range from roughly $140K to $1.65M. The distribution is slightly right-skewed, indicating a tail of higher-priced flats. Lower-priced units cluster more tightly around the median. A standard deviation of about $182K shows how much prices typically deviate from the mean. The interquartile range (middle 50 % of transactions) is roughly $240K.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute skewness of resale price\n",
    "skewness = df['resale_price'].skew()\n",
    "print(f\"Skewness of Resale Price: {skewness:.2f}\")\n",
    "if skewness > 0.5: # type: ignore[arg-type]\n",
    "    print(\"The distribution is positively skewed.\")\n",
    "elif skewness < -0.5: # type: ignore[arg-type]\n",
    "    print(\"The distribution is negatively skewed.\")\n",
    "else:\n",
    "    print(\"The distribution is approximately symmetric.\")\n",
    "\n",
    "# Compute kurtosis of resale price\n",
    "kurtosis = df['resale_price'].kurt()\n",
    "print(f\"Kurtosis of Resale Price: {kurtosis:.2f}\")\n",
    "if kurtosis > 3: # type: ignore[arg-type]\n",
    "    print(\"The distribution is leptokurtic.\")\n",
    "elif kurtosis < 3: # type: ignore[arg-type]\n",
    "    print(\"The distribution is platykurtic.\")\n",
    "else:\n",
    "    print(\"The distribution is mesokurtic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on a skewness of 0.95, the HDB resale-price distribution is moderately right-skewed, indicating some high-price outliers are pulling the mean above the median. Although below 1.0, this skewness can still affect models that assume normal residuals, so we should check residual plots (e.g., residuals vs. fitted, Q–Q) and consider a log or Box–Cox transformation if needed.**\n",
    "\n",
    "**With a kurtosis of 1.15 (< 3), the distribution is platykurtic, meaning it’s flatter-topped with thinner tails than a normal curve—there are less extreme values since the tail is thinner than those under Gaussian assumptions**\n",
    "\n",
    "**Together, these measures reveal a fairly even spread of resale prices with a moderate right tail. For predictive modeling, we recommend running residual diagnostics and experimenting with a log(price) transform or robust regression techniques to ensure our assumptions hold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.investopedia.com/terms/p/platykurtic.asp#:~:text=The%20term%20%22platykurtic%22%20refers%20to,extreme%20positive%20or%20negative%20events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers detection using IQR \n",
    "resale_price_iqr_q1 = df['resale_price'].quantile(0.25)\n",
    "resale_price_iqr_q3 = df['resale_price'].quantile(0.75)\n",
    "\n",
    "resale_price_iqr_outliers_left = df[(df['resale_price'] < resale_price_iqr_q1 - 1.5 * resale_price_iqr)]\n",
    "print(f'The number of outliers (IQR) in the left distribution of resale price is {len(resale_price_iqr_outliers_left)}')\n",
    "resale_price_iqr_outliers_right = df[(df['resale_price'] > resale_price_iqr_q3 + 1.5 * resale_price_iqr)]\n",
    "print(f'The number of outliers (IQR) in the right distribution of resale price is {len(resale_price_iqr_outliers_right)}')\n",
    "\n",
    "resale_price_iqr_outliers = pd.concat([resale_price_iqr_outliers_left,resale_price_iqr_outliers_right], ignore_index=True, axis=0)\n",
    "print(f'The total number of outliers (IQR) in the resale price is {len(resale_price_iqr_outliers)}')\n",
    "\n",
    "# percentage of outliers in the resale price\n",
    "resale_price_iqr_outliers_percentage = len(resale_price_iqr_outliers) / len(df) * 100\n",
    "print(f'Total number of transactions is {len(df)}')\n",
    "print(f'The percentage of outliers (IQR) in the resale price is {resale_price_iqr_outliers_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers detection using Z-score\n",
    "resale_price_z_score = (df['resale_price'] - df['resale_price'].mean()) / df['resale_price'].std()\n",
    "resale_price_z_score_threshold = 3\n",
    "resale_price_z_score_outliers_left = df[resale_price_z_score < np.negative(resale_price_z_score_threshold)]\n",
    "print(f'The number of outliers (z-score) in the left distribution of resale price is {len(resale_price_z_score_outliers_left)}')\n",
    "resale_price_z_score_outliers_right = df[resale_price_z_score > resale_price_z_score_threshold]\n",
    "print(f'The number of outliers (z-score) in the right distribution of resale price is {len(resale_price_z_score_outliers_right)}')\n",
    "\n",
    "resale_price_z_score_outliers = pd.concat([resale_price_z_score_outliers_left, resale_price_z_score_outliers_right], axis=0, ignore_index=True)\n",
    "print(f'The total number of outliers (z-score) in the resale price is {len(resale_price_z_score_outliers)}')\n",
    "\n",
    "# percentage of outliers in the resale price\n",
    "resale_price_z_score_outliers_percentage = len(resale_price_z_score_outliers) / len(df) * 100\n",
    "print(f'Total number of transactions is {len(df)}')\n",
    "print(f'The percentage of outliers (z-score) in the resale price is {resale_price_z_score_outliers_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using IQR method, there are 1.74% of transactions lie beyond 1.5 times of the third quartiles, so extreme HDB resale prices are fairly uncommon but not very rare.**\n",
    "\n",
    "**Using z-score method, there are 0.84% of transactions lie beyond +3 $\\sigma$. Compared to a Gaussian distribution, where 0.27% of data lies beyond ±3 σ, this 0.84% indicates a heavier right tail than expected under normal assumptions, suggesting the presence of significant high-value transactions, despite the overall platykurtic nature of the distribution (which implies thinner tails in general compared to a normal curve).** \n",
    "\n",
    "**The reason, z-score method detect less outliers because extreme values impacted its means and standard deviation. With elevated means and standard deviation, z-score method is only able to detect very extreme values. In a skewed distribution, the better method is using IQR method, as it is less sensitive to outliers. We will be adopting the percentage of outliers as 1.74% based on IQR method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of outliers\n",
    "print(\"Descriptive statistics of detected outliers:\")\n",
    "print(resale_price_iqr_outliers['resale_price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the outliers of resale price\n",
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.histplot(data=resale_price_iqr_outliers, x='resale_price', kde=True)\n",
    "\n",
    "\n",
    "ax.set_title('Distribution of Resale Price Outliers')\n",
    "ax.set_xlabel('Resale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot the distribution of the outliers of resale price\n",
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.boxplot(data=resale_price_iqr_outliers, x='resale_price', vert=False)\n",
    "\n",
    "\n",
    "ax.set_title('Boxplot of Resale Price Outliers')\n",
    "ax.set_xlabel('Resale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even among the 3,643 detected outliers, the distribution of resale prices is right-skewed (mean $1.098M vs. median $1.06M). This indicates that within this outlier segment, there are still a few significantly higher-value transactions that exert disproportional influence on this segment's mean and standard deviation.**\n",
    "\n",
    "**These most extreme transactions within the outlier group are the ones that will have the greatest impact on the overall mean and standard deviation of the full HDB resale price dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Million Dollar Flats by Year (Outliers in Layman Conext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute million dollars flats by year using a separate df\n",
    "million_dollars_flats = df[df['resale_price'] >= 1000000].copy()\n",
    "million_dollars_flats['month'] = pd.to_datetime(million_dollars_flats['month'])\n",
    "million_dollars_flats['year'] = million_dollars_flats['month'].dt.year\n",
    "million_dollars_flats_by_year = million_dollars_flats.groupby('year').size()\n",
    "million_dollars_flats_by_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the million dollars flats by year\n",
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.barplot(x=million_dollars_flats_by_year.index, y=million_dollars_flats_by_year.values)\n",
    "\n",
    "ax.set_title('Billion Dollars Flats by Year')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Flats')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "million_dollars_flats.loc[million_dollars_flats['resale_price'].sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the outliers analysis above, we have the following strategies:\n",
    "\n",
    "**Method 1 : Transform the target (or rely on Huber regression)**\n",
    "\n",
    "Apply a log or Box-Cox transform so resale prices approximate a normal distribution. This stabilizes variance and usually trims RMSE, but back-transforming compresses the right tail, so million-dollar flats are systematically under-predicted. Skipping the transform and fitting a Huber regressor instead keeps the target in dollars while down-weighting extreme residuals via its piece-wise (quadratic ↦ linear) loss.\n",
    "\n",
    "<sub>Refs: Box & Cox 1964; Huber 1964.</sub>\n",
    "\n",
    "**Method 2 : Winsorise the most extreme outliers**\n",
    "\n",
    "Cap, rather than delete, the top (say) 0.5 % of sale prices at the 99.5th percentile. Our leverage plot shows a handful of points far beyond the main tail; capping them removes their undue influence while preserving rank information.\n",
    "\n",
    "<sub>Ref: Tukey 1962.</sub>\n",
    "\n",
    "**Method 3: Segment the market and fit specialized models**\n",
    "\n",
    "Train a classifier that flags \"premium-location\" transactions (e.g. postal code, distance to CBD). Fit separate regressors for the luxury and mainstream segments, then stack or blend their predictions. The luxury regressor can learn high-end drivers (view corridor, penthouse level) that a single global model would treat as noise.\n",
    "\n",
    "<sub>Ref: Breiman 1996, “Stacked Regressions”.</sub>\n",
    "\n",
    "**Method 4: Quantile regression**\n",
    "\n",
    "Minimize the pinball loss at chosen quantiles (e.g. 50th, 90th). Because the method doesn’t assume Gaussian errors, it is naturally robust to heavy tails and produces prediction intervals without bootstrapping.\n",
    "\n",
    "<sub>Ref: Koenker & Bassett 1978.</sub>\n",
    "\n",
    "**Tree-based ensembles (Random Forest, Gradient Boosting, XGBoost/LightGBM)**\n",
    "\n",
    "Decision trees split on features, so a handful of ultra-high prices are isolated in tiny terminal nodes instead of skewing every split. Ensembles average (Random Forest) or stage-wise correct (GBM) those trees, giving high bias-variance control and built-in outlier tolerance.\n",
    "\n",
    "Pros:\n",
    "- Capture complex non-linearities and interactions without manual feature engineering.\n",
    "- Handle heterogeneous variance (“price heteroskedasticity”) gracefully.\n",
    "\n",
    "Cons:\n",
    "- Still optimize squared-error inside each leaf, so extreme targets inside the same leaf can inflate loss; tune max_depth and min_samples_leaf to avoid this.\n",
    "- Feature importance can be biased toward high-cardinality categorical splits; apply permutation importance or SHAP to audit.\n",
    "\n",
    "<sub>Refs: Breiman 2001 (Random Forest); Friedman 2001 (GBM); Chen & Guestrin 2016 (XGBoost); Ke et al. 2017 (LightGBM).</sub>\n",
    "\n",
    "**We will perform the aforementioned methods during model selection.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis (Univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = numerical_columns[:2]\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical_column(df: pd.DataFrame, each_column: str):\n",
    "    \"\"\"    Feature Analysis of Numerical Columns\n",
    "    \"\"\"\n",
    "    print('========================================')\n",
    "    print('Feature Analysis of', each_column)\n",
    "    print('========================================')\n",
    "    print(df[each_column].describe())\n",
    "\n",
    "    # plot the distribution of the column\n",
    "    fig, ax = plt.subplots(figsize=(8,5)) \n",
    "    sns.histplot(data=df, x=each_column, kde=True)\n",
    "    \n",
    "    ax.set_title(f'Distribution of {each_column}')\n",
    "    ax.set_xlabel(each_column)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute skewness of floor_area_sqm\n",
    "    skewness = df[each_column].skew()\n",
    "    print(f\"Skewness of {each_column}: {skewness:.2f}\")\n",
    "    if skewness > 0.5: # type: ignore[arg-type]\n",
    "        print(\"The distribution is positively skewed.\")\n",
    "    elif skewness < -0.5: # type: ignore[arg-type]\n",
    "        print(\"The distribution is negatively skewed.\")\n",
    "    else:\n",
    "        print(\"The distribution is approximately symmetric.\")\n",
    "\n",
    "    # Compute kurtosis of resale price\n",
    "    kurtosis = df[each_column].kurt()\n",
    "    print(f\"Kurtosis of {each_column}: {kurtosis:.2f}\")\n",
    "    if kurtosis > 3: # type: ignore[arg-type]\n",
    "        print(\"The distribution is leptokurtic.\")\n",
    "    elif kurtosis < 3: # type: ignore[arg-type]\n",
    "        print(\"The distribution is platykurtic.\")\n",
    "    else:\n",
    "        print(\"The distribution is mesokurtic.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Feature Column: Floor Area (square meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numerical_column(df, 'floor_area_sqm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While the skewness of `floor_area_sqm` is low at 0.26, statistically suggesting an approximately symmetric distribution, visual inspection of the data reveals a long extension to the very right, with a maximum value of 366.7 sqm significantly higher than the 75th percentile of 112 sqm.**\n",
    "\n",
    "**The relatively low skewness, despite this visible right tail, is likely due to the bulk of the distribution being quite symmetric around its central tendency, thus tempering the overall skewness metric. Furthermore, the likely multimodal nature of floor_area_sqm (reflecting distinct HDB flat types/sizes, such as 3-room, 4-room, 5-room, and executive flats) also contributes to lowering the aggregate skewness.**\n",
    "\n",
    "**For linear regression models, we will not apply a direct transformation to floor_area_sqm based on its low overall skewness. However, we will carefully examine residual plots to ensure assumptions of linearity and homoscedasticity hold, as the long right tail and distinct modes could still subtly influence model behavior.**\n",
    "\n",
    "**Conversely, tree-based models (such as Random Forest or Gradient Boosting) are inherently well-suited to handle multimodal feature distributions without requiring explicit transformations. Their ability to make splits based on thresholds allows them to naturally segment and learn from distinct peaks in the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Lease Commence Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numerical_column(df, 'lease_commence_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness of `lease_commerce_date` is low at 0.07, suggesting an symmetric distribution. Visual inspection of the chart reveals a multimodal distributions.**\n",
    "\n",
    "**We will consider tree-based models (such as Random Forest or Gradient Boosting) are inherently well-suited to handle multimodal feature distributions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_column(df: pd.DataFrame, column_name: str, rare_threshold: float = 0.01): # 0.01 (1%) is the default threshold for rare values\n",
    "    \"\"\"\n",
    "    Display the statistics of the column\n",
    "    Countplot of the column sorted by the value counts with highest value counts on top (horizontal view)\n",
    "    Value counts sorted by the value counts with weights in percentage\n",
    "    \"\"\"\n",
    "    # Display the column name\n",
    "    print('========================================')\n",
    "    print(column_name)\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Display the statistics of the column\n",
    "    print(f'{column_name} describe:')\n",
    "    print(df[column_name].describe())\n",
    "\n",
    "    print('--------------------------------')\n",
    "    # countplot of the column sorted by the value counts with highest value counts on top\n",
    "    fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.countplot(data=df, y=column_name, order=df[column_name].value_counts().index)\n",
    "\n",
    "    ax.set_title(f'Count of {column_name}')\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_ylabel(column_name)\n",
    "    ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation = 90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display the value counts with percentage\n",
    "    value_counts = df[column_name].value_counts(dropna=False)\n",
    "    value_counts_percentage = value_counts/len(df[column_name])\n",
    "    value_count_rare_df = pd.DataFrame({\n",
    "        \"value_count\"   : value_counts,\n",
    "        \"percent\" : value_counts_percentage\n",
    "    })\n",
    "    value_count_rare_df[\"is_rare\"] = value_count_rare_df[\"percent\"] < rare_threshold\n",
    "\n",
    "    print('--------------------------------')\n",
    "    print(f'Rare threshold of {rare_threshold * 100}%')\n",
    "    print(value_count_rare_df)\n",
    "    print('========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_column(df,'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The feature `month`, need to convert to datetime format. However, ML models could not interpret datetime, therefore we need to convert them to numerical features in `transaction_year` and `transaction_month`. We will use `transaction_year` as ordinal numerical features. For `transaction_month`, this is seasonal, therefore we can either use one-hot encoding or cyclic sine/cosine pair (two columns that preserve the circular nature).**\n",
    "\n",
    "**To simplified feature engineering, we will use one-hot encoding for `transaction_month`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_month_to_year_month(df: pd.DataFrame, \n",
    "                              month: str = 'month',\n",
    "                              transaction_year: str = 'transaction_year',\n",
    "                              transaction_month: str = 'transaction_month') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert month column to separate year and month columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe containing the month column\n",
    "    month : str, default 'month'\n",
    "        Name of the column containing month data\n",
    "    transaction_year : str, default 'transaction_year'\n",
    "        Name of the output year column\n",
    "    month_out_col : str, default 'transaction_month'\n",
    "        Name of the output month column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Modified dataframe\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If month_col doesn't exist in the dataframe\n",
    "    TypeError\n",
    "        If df is not a pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "    \n",
    "    # Check if dataframe is empty\n",
    "    if df.empty:\n",
    "        warnings.warn(\"Input dataframe is empty\", UserWarning)\n",
    "        return df.copy()\n",
    "    \n",
    "    # Check if month column exists\n",
    "    if month not in df.columns:\n",
    "        raise ValueError(f\"Column '{month}' not found in dataframe\")\n",
    "    \n",
    "    # Work on copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert to datetime with error handling\n",
    "    try:\n",
    "        df[month] = pd.to_datetime(df[month])\n",
    "    except (ValueError, TypeError) as e:\n",
    "        raise ValueError(f\"Cannot convert column '{month}' to datetime: {str(e)}\")\n",
    "    \n",
    "    # Extract year and month\n",
    "    df[transaction_year] = df[month].dt.year\n",
    "    df[transaction_month] = df[month].dt.month\n",
    "    \n",
    "    # Return dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLE TESTS - Run these in your notebook\n",
    "# =============================================================================\n",
    "\n",
    "def test_basic_functionality():\n",
    "    \"\"\"Test that the function works with normal data\"\"\"\n",
    "    # Create test data\n",
    "    test_df = pd.DataFrame({\n",
    "        'month': ['2023-01-15', '2023-02-20', '2023-03-10'],\n",
    "        'value': [100, 200, 300]\n",
    "    })\n",
    "    \n",
    "    # Run function\n",
    "    result = convert_month_to_year_month(test_df)\n",
    "    \n",
    "    # Check results\n",
    "    assert 'transaction_year' in result.columns\n",
    "    assert 'transaction_month' in result.columns\n",
    "    assert result['transaction_year'].tolist() == [2023, 2023, 2023]\n",
    "    assert result['transaction_month'].tolist() == [1, 2, 3]\n",
    "    \n",
    "    print(\"✓ Basic functionality test passed!\")\n",
    "\n",
    "\n",
    "test_basic_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_different_date_formats():\n",
    "    \"\"\"Test with different date formats\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        'month': ['2023-01', '2023-02', '2023-03'],\n",
    "        'value': [100, 200, 300]\n",
    "    })\n",
    "    \n",
    "    result = convert_month_to_year_month(test_df)\n",
    "    \n",
    "    assert result['transaction_year'].iloc[0] == 2023\n",
    "    assert result['transaction_month'].iloc[0] == 1\n",
    "    \n",
    "    print(\"✓ Different date formats test passed!\")\n",
    "\n",
    "test_different_date_formats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_with_nulls():\n",
    "    \"\"\"Test with null values\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        'month': ['2023-01-15', None, '2023-03-10'],\n",
    "        'value': [100, 200, 300]\n",
    "    })\n",
    "    \n",
    "    result = convert_month_to_year_month(test_df)\n",
    "    \n",
    "    # First and third rows should work\n",
    "    assert result['transaction_year'].iloc[0] == 2023\n",
    "    assert result['transaction_month'].iloc[0] == 1\n",
    "    \n",
    "    print(\"✓ Null values test passed!\")\n",
    "\n",
    "test_with_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_original_unchanged():\n",
    "    \"\"\"Test that original dataframe is not modified\"\"\"\n",
    "    original_df = pd.DataFrame({\n",
    "        'month': ['2023-01-15', '2023-02-20'],\n",
    "        'value': [100, 200]\n",
    "    })\n",
    "    \n",
    "    # Store original state\n",
    "    original_columns = original_df.columns.tolist()\n",
    "    \n",
    "    # Run function\n",
    "    result = convert_month_to_year_month(original_df)\n",
    "    \n",
    "    # Check original is unchanged\n",
    "    assert original_df.columns.tolist() == original_columns\n",
    "    assert 'transaction_year' not in original_df.columns\n",
    "    \n",
    "    # Check result has new columns\n",
    "    assert 'transaction_year' in result.columns\n",
    "    \n",
    "    print(\"✓ Original unchanged test passed!\")\n",
    "\n",
    "test_original_unchanged()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_column(df,'town')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The feature `town` has 26 unique values. Since town is a nominal categorical variable and not ordinal, we will use one-hot encoding. The top 3 towns with the highest transaction turnover are SENGKANG, PUNGGOL, and WOODLANDS.**\n",
    "\n",
    "**The towns CENTRAL AREA, MARINE PARADE, and BUKIT TIMAH have transaction counts that are less than 1% of the total transactions. Due to their rare occurrence, these categories might potentially impact predictive power, particularly for models sensitive to sparsity. One option to address this could be to group these three towns into a single \"RARE\" or \"PREMIUM\" category to potentially improve predictive capabilities.**\n",
    "\n",
    "**However, grouping them may distort valuable information if there are changes to the trends or characteristics of these specific towns in future data. Therefore, we have decided to keep all towns as separate categories with one-hot encoding. We will be using tree-based models, which are robust to rare categories and high-cardinality features, to handle these less frequent occurrences effectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8508d",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Flat type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_column(df, 'flat_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f1175",
   "metadata": {},
   "source": [
    "**Analysis of `flat_type` Feature:**\n",
    "\n",
    "**The `flat_type` feature contains 7 unique categories. The most popular resale flat types are 4 ROOM (42.27%), followed by 5 ROOM (24.63%) and 3 ROOM (23.80%), collectively accounting for the vast majority of transactions.**\n",
    "\n",
    "**Given the inherent ordinality of flat types (e.g., 1-room, 2-room, 3-room, 4-room, 5-room, Executive, Multi-Generation typically representing increasing size), we may consider ordinal encoding for models that can leverage this ordered relationship, such as linear models.** \n",
    "\n",
    "**However, we discovered that there are exceptions and outliers in 3-room flat with floor area more than 200sqm. This is unusual. We found that during the 1960s, there are terrace flat model which are categorized as 3-room flat, these flats has very high floor area that are more than 200sqm. HDB has stop building such flat. Given the new information, we think that it is best to use one-hot encoding as it avoids assumptions about the linearity of the ordinal relationship.** \n",
    "\n",
    "**The MULTI-GENERATION and 1 ROOM flat types each represent less than 0.04% of total transactions, falling below our 1.0% rare threshold. We have decided not to group these sparse categories into an 'Other' type. This is because despite their rare occurrence, these specific flat types likely contain valuable and distinct information (e.g., 1-room flats typically being the smallest/lowest priced, and multi-generation flats being unique, large units). To effectively handle these rare occurrences and capture their potentially unique impact without distortion, we will rely on robust model-level smoothing provided by ensemble models (e.g., Random Forest, Gradient Boosting), which are well-suited for sparse categorical features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Storey Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_column(df, 'storey_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given its ordinarity nature, we will convert the categories from string to numerical number. We do that by extracting the numerical number and take the average. For example, if storey range is from 7th floor to 9th floor. We will take the middle floor, 8th floor as the indicator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_storey_range(storey_range):\n",
    "    \"\"\"\n",
    "    Convert storey range to the numerical average.\n",
    "    Args:\n",
    "        storey_range in (str)\n",
    "\n",
    "    Returns: \n",
    "        float\n",
    "\n",
    "    Example:\n",
    "        convert_storey_range('07 TO 09') -> 8.0 \n",
    "    \"\"\"\n",
    "\n",
    "    low, high = storey_range.split(' TO ')\n",
    "    average = (int(low) + int(high)) / 2\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_storey_range():\n",
    "    \"\"\"Test cases for convert_storey_range function\"\"\"\n",
    "    \n",
    "    # Basic test case from the example\n",
    "    assert convert_storey_range('07 TO 09') == 8.0\n",
    "    \n",
    "    # Test with single digits\n",
    "    assert convert_storey_range('1 TO 3') == 2.0\n",
    "    assert convert_storey_range('5 TO 7') == 6.0\n",
    "    \n",
    "    # Test with same storey (no range)\n",
    "    assert convert_storey_range('05 TO 05') == 5.0\n",
    "    assert convert_storey_range('10 TO 10') == 10.0\n",
    "    \n",
    "    # Test with larger ranges\n",
    "    assert convert_storey_range('01 TO 05') == 3.0\n",
    "    assert convert_storey_range('10 TO 20') == 15.0\n",
    "    \n",
    "    # Test with double digits\n",
    "    assert convert_storey_range('12 TO 16') == 14.0\n",
    "    assert convert_storey_range('25 TO 35') == 30.0\n",
    "    \n",
    "    # Test with leading zeros\n",
    "    assert convert_storey_range('01 TO 03') == 2.0\n",
    "    assert convert_storey_range('08 TO 12') == 10.0\n",
    "    \n",
    "    # Test odd ranges (result should be .5)\n",
    "    assert convert_storey_range('1 TO 2') == 1.5\n",
    "    assert convert_storey_range('10 TO 11') == 10.5\n",
    "    \n",
    "    # Test with higher floors\n",
    "    assert convert_storey_range('50 TO 60') == 55.0\n",
    "    assert convert_storey_range('99 TO 101') == 100.0\n",
    "\n",
    "    print(\"All test cases passed!\")\n",
    "\n",
    "\n",
    "test_convert_storey_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_convert_storey_range_edge_cases():\n",
    "    \"\"\"Test edge cases and potential error scenarios\"\"\"\n",
    "    \n",
    "    # Test ground floor scenarios\n",
    "    assert convert_storey_range('0 TO 2') == 1.0\n",
    "    assert convert_storey_range('00 TO 01') == 0.5\n",
    "    \n",
    "    # Test with mixed formatting\n",
    "    assert convert_storey_range('5 TO 15') == 10.0\n",
    "    assert convert_storey_range('02 TO 8') == 5.0\n",
    "\n",
    "    print(\"All edge case tests passed!\")\n",
    "\n",
    "test_convert_storey_range_edge_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytest\n",
    "def test_convert_storey_range_errors():\n",
    "    \"\"\"Test error handling scenarios\"\"\"\n",
    "    \n",
    "    # Test invalid format (should raise ValueError)\n",
    "    with pytest.raises(ValueError):\n",
    "        convert_storey_range('invalid format')\n",
    "    \n",
    "    with pytest.raises(ValueError):\n",
    "        convert_storey_range('5-7')  # Wrong separator\n",
    "    \n",
    "    with pytest.raises(ValueError):\n",
    "        convert_storey_range('5 TO')  # Missing high value\n",
    "    \n",
    "    with pytest.raises(ValueError):\n",
    "        convert_storey_range('TO 7')  # Missing low value\n",
    "    \n",
    "    # Test non-numeric values\n",
    "    with pytest.raises(ValueError):\n",
    "        convert_storey_range('A TO B')\n",
    "    \n",
    "    with pytest.raises(ValueError):\n",
    "        convert_storey_range('1 TO B')\n",
    "\n",
    "    print(\"All error handling tests passed!\")\n",
    "\n",
    "test_convert_storey_range_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternative test format using unittest if you prefer\n",
    "import unittest\n",
    "\n",
    "class TestConvertStoreyRange(unittest.TestCase):\n",
    "    \n",
    "    def test_basic_functionality(self):\n",
    "        \"\"\"Test basic functionality\"\"\"\n",
    "        self.assertEqual(convert_storey_range('07 TO 09'), 8.0)\n",
    "        self.assertEqual(convert_storey_range('1 TO 3'), 2.0)\n",
    "        self.assertEqual(convert_storey_range('10 TO 20'), 15.0)\n",
    "        print(\"Basic functionality test passed!\")\n",
    "    \n",
    "    def test_same_storey(self):\n",
    "        \"\"\"Test when low and high are the same\"\"\"\n",
    "        self.assertEqual(convert_storey_range('05 TO 05'), 5.0)\n",
    "        self.assertEqual(convert_storey_range('10 TO 10'), 10.0)\n",
    "        print(\"Same storey test passed!\")\n",
    "    \n",
    "    def test_decimal_results(self):\n",
    "        \"\"\"Test cases that result in decimal values\"\"\"\n",
    "        self.assertEqual(convert_storey_range('1 TO 2'), 1.5)\n",
    "        self.assertEqual(convert_storey_range('10 TO 11'), 10.5)\n",
    "        print(\"Decimal results test passed!\")\n",
    "    \n",
    "    def test_invalid_input(self):\n",
    "        \"\"\"Test invalid input handling\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            convert_storey_range('invalid')\n",
    "        with self.assertRaises(ValueError):\n",
    "            convert_storey_range('A TO B')\n",
    "        print(\"Invalid input test passed!\")\n",
    "\n",
    "\n",
    "unit_test = TestConvertStoreyRange()\n",
    "unit_test.test_basic_functionality()\n",
    "unit_test.test_same_storey()\n",
    "unit_test.test_decimal_results()\n",
    "unit_test.test_invalid_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.storey_range = df.storey_range.apply(convert_storey_range)\n",
    "df.storey_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.storey_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.storey_range.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This feature column has been convert as numerical columns as it contains ordinality and we can take the average as the reference indicator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Flat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_column(df, 'flat_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are more sparse categories, however, each categories contains its unique information which maybe helpful in differentiating between different type of flats with different model and its different pricing. We will use one-hot encoding since there is no ordinarity in this feature.**\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Maisonette and Premium Maisonette are duplex units.\n",
    "- DBSS (Design, Build and Sell Scheme) flats were built by private developers and often have condo-like features.\n",
    "- Terrace flats are the specific, large 3-room flats build in the 1960s.\n",
    "- Multi Generation and 3Gen are specific multi-generational living units.\n",
    "\n",
    "**These are not just \"rare\" in frequency but \"rare\" in their type and market position. Grouping them into an \"Other\" category would likely lead to a significant loss of valuable information.**\n",
    "\n",
    "**We will be using tree-based ensemble model to handle the sparsity of the features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Feature Column: Remaining Lease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_column(df, 'remaining_lease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With 690 categories, we consider this feature has high cardinality. Thus we will convert this feature from string to numerical column. We will convert the remaining lease into a new column `remaining_lease_by_month`. We will convert the years into months and add to the remaining months. For example, `94 years 10 months` will be converted to 1138 months.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lease_to_month(lease):\n",
    "    \"\"\"\n",
    "    Convert remaining lease period from string to total number of months.\n",
    "    Args:\n",
    "        remaining_lease in (str)\n",
    "\n",
    "    Returns: \n",
    "        integer\n",
    "\n",
    "    Example:\n",
    "        convert_lease_to_month('07 TO 09') -> 8.0  \n",
    "    \"\"\"\n",
    "    str_list = lease.split(' ')\n",
    "    if ('months' in str_list) | ('month' in str_list):\n",
    "        year = int(str_list[0])\n",
    "        month = int(str_list[2])\n",
    "        t_month = (year * 12) + month \n",
    "    elif ('years' in str_list) & (('months' not in str_list) | ('month' not in str_list)):\n",
    "        year = int(str_list[0])\n",
    "        t_month = (year * 12)\n",
    "    else:\n",
    "        year = int(str_list[0])\n",
    "        t_month = (year * 12)        \n",
    "    return t_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_convert_lease_to_month_simple():\n",
    "    \"\"\"Simple test cases for convert_lease_to_month function\"\"\"\n",
    "    \n",
    "    # Test years and months format\n",
    "    assert convert_lease_to_month('5 years 6 months') == 66  # 5*12 + 6 = 66\n",
    "    assert convert_lease_to_month('2 years 3 months') == 27  # 2*12 + 3 = 27\n",
    "    assert convert_lease_to_month('1 years 0 months') == 12  # 1*12 + 0 = 12\n",
    "    \n",
    "    # Test years only format\n",
    "    assert convert_lease_to_month('10 years') == 120  # 10*12 = 120\n",
    "    assert convert_lease_to_month('5 years') == 60    # 5*12 = 60\n",
    "    assert convert_lease_to_month('1 years') == 12    # 1*12 = 12\n",
    "    \n",
    "    # Test with singular 'month'\n",
    "    assert convert_lease_to_month('3 years 1 month') == 37   # 3*12 + 1 = 37\n",
    "    assert convert_lease_to_month('0 years 1 month') == 1    # 0*12 + 1 = 1\n",
    "    \n",
    "    # Test just numbers (should default to years)\n",
    "    assert convert_lease_to_month('5') == 60    # 5*12 = 60\n",
    "    assert convert_lease_to_month('2') == 24    # 2*12 = 24\n",
    "    print(\"All simple test cases passed!\")\n",
    "\n",
    "test_convert_lease_to_month_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_convert_lease_to_month_variations():\n",
    "    \"\"\"Test various input formats\"\"\"\n",
    "    \n",
    "    # Test different plural/singular combinations\n",
    "    assert convert_lease_to_month('1 year 1 month') == 13    # Should work if 'year' -> 'years'\n",
    "    assert convert_lease_to_month('10 years 11 months') == 131  # 10*12 + 11 = 131\n",
    "    \n",
    "    # Test zero cases\n",
    "    assert convert_lease_to_month('0 years 6 months') == 6   # 0*12 + 6 = 6\n",
    "    assert convert_lease_to_month('0 years') == 0      \n",
    "    print(\"All variations test cases passed!\")\n",
    "\n",
    "test_convert_lease_to_month_variations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['remaining_lease_by_month'] = df.remaining_lease.apply(convert_lease_to_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['remaining_lease_by_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By converting this feature from string with year and months into numerical months, we can better manage this feature and this feature column also carry important information of the remaining live of a flat that may impacted the resale price.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis: Feature vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture updated numerical and categorical columns\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = numerical_columns.drop('resale_price')\n",
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'resale_price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical_features_with_target(df: pd.DataFrame,\n",
    "                                        numerical_column: str,\n",
    "                                        target: str = \"resale_price\"):\n",
    "    \"\"\"\n",
    "    Extended numeric-vs-target EDA:\n",
    "    • scatter + LOWESS smooth\n",
    "    • regplot with linear fit\n",
    "    • Pearson & Spearman correlations\n",
    "    • residual plot (checks homoskedasticity / curvature)\n",
    "    • optional log-target & price-per-sqm transforms\n",
    "    \"\"\"\n",
    "    import seaborn as sns, matplotlib.pyplot as plt\n",
    "    import numpy as np, scipy.stats as st\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # ---------- 0. header ----------\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Feature: {numerical_column}   ↔   Target: {target}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # ---------- 1. scatter + LOWESS ----------\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(data=df, x=numerical_column, y=target, alpha=0.25, ax=ax)\n",
    "\n",
    "    # LOWESS curve (locally-weighted regression)\n",
    "    lowess = sm.nonparametric.lowess\n",
    "    lo = lowess(df[target], df[numerical_column], frac=0.3, return_sorted=True)\n",
    "    ax.plot(lo[:, 0], lo[:, 1], lw=2, label=\"LOWESS\")\n",
    "    ax.set_title(f\"{numerical_column} vs {target} with LOWESS\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- 2. regplot (linear fit) ----------\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.regplot(data=df, x=numerical_column, y=target,\n",
    "                scatter_kws={\"alpha\": 0.15},\n",
    "                line_kws={\"lw\": 2}, ax=ax)\n",
    "    ax.set_title(f\"Linear fit of {target} on {numerical_column}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- 3. Residual plot ----------\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.residplot(data=df, x=numerical_column, y=target,\n",
    "                  lowess=True, scatter_kws={\"alpha\": 0.2}, ax=ax)\n",
    "    ax.set_title(f\"Residuals: {target} ~ {numerical_column}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- 4. Correlation metrics ----------\n",
    "    pearson_r, pearson_p = st.pearsonr(df[numerical_column], df[target])\n",
    "    spearman_r, spearman_p = st.spearmanr(df[numerical_column], df[target], nan_policy=\"omit\")\n",
    "\n",
    "    print(f\"Pearson r = {pearson_r:.3f}  (p={pearson_p:.3g})\")\n",
    "    print(f\"Spearman ρ = {spearman_r:.3f}  (p={spearman_p:.3g})\")\n",
    "\n",
    "    # ---------- 5. Simple transform checks ----------\n",
    "    # A. Log-price\n",
    "    if (df[target] > 0).all():\n",
    "        df[\"_log_price\"] = np.log(df[target])\n",
    "        log_corr = df[[numerical_column, \"_log_price\"]].corr(method=\"pearson\").iloc[0, 1]\n",
    "        print(f\"Pearson r vs log({target}) = {log_corr:.3f}\")\n",
    "\n",
    "    # B. Price per sqm if the column is floor area\n",
    "    if numerical_column.lower().startswith(\"floor_area\"):\n",
    "        df[\"_price_psm\"] = df[target] / df[numerical_column]\n",
    "        ppsm_corr = df[[\"_price_psm\", numerical_column]].corr(method=\"pearson\").iloc[0, 1]\n",
    "        print(f\"Corr(floor_area, price_per_sqm) = {ppsm_corr:.3f}\")\n",
    "\n",
    "    print(\"\")  # spacer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Feature Column: Storey Range with HDB Resale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numerical_features_with_target(df, 'storey_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['storey_range'].corr(df['resale_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "pearson_r,  p1 = stats.pearsonr(df['storey_range'], df['resale_price'])\n",
    "spearman_r, p2 = stats.spearmanr(df['storey_range'], df['resale_price'])\n",
    "print(f\"Pearson r = {pearson_r:.3f}, p = {p1:.3g}\")\n",
    "print(f\"Spearman ρ = {spearman_r:.3f}, p = {p2:.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Log-price to curb spread\n",
    "log_price = np.log(df['resale_price'])\n",
    "\n",
    "r_log, _ = stats.pearsonr(df['storey_range'], log_price)\n",
    "rho_log,_ = stats.spearmanr(df['storey_range'], log_price)\n",
    "\n",
    "print(f\"Pearson r (log-price)   = {r_log:.3f}\")\n",
    "print(f\"Spearman ρ (log-price)  = {rho_log:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,5))\n",
    "sns.boxplot(x='storey_range', y='resale_price', data=df, showfliers=False)\n",
    "sns.stripplot(x='storey_range', y='resale_price', data=df,\n",
    "              alpha=.3, jitter=.15, color='black', size=2)\n",
    "plt.xticks(rotation=90); plt.title(\"Price distribution per storey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread = (\n",
    "    df.groupby('storey_range')['resale_price']\n",
    "      .agg(median='median',\n",
    "           IQR   =lambda s: s.quantile(0.75) - s.quantile(0.25),\n",
    "           std   ='std',\n",
    "           count ='size')\n",
    ")\n",
    "spread.plot(y='IQR', figsize=(8,4), title='IQR of price by floor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "lowess = sm.nonparametric.lowess\n",
    "smoothed = lowess(df['resale_price'], df['storey_range'], frac=0.3)\n",
    "plt.scatter(df['storey_range'], df['resale_price'], s=8, alpha=.2)\n",
    "plt.plot(smoothed[:,0], smoothed[:,1], lw=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.vander(df['storey_range'], 3)     # quadratic\n",
    "coef = np.linalg.lstsq(X, df['resale_price'], rcond=None)[0]\n",
    "pred = X @ coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = df['resale_price'] - pred\n",
    "sns.residplot(x=df['storey_range'], y=resid, lowess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, scipy.stats as st\n",
    "df['floor_bin'] = pd.cut(df['storey_range'], [0,20,40,60],\n",
    "                         labels=['low','mid','high'])\n",
    "anova_F, anova_p = st.f_oneway(*(df.loc[df.floor_bin==b,'resale_price']\n",
    "                                 for b in df.floor_bin.unique()))\n",
    "print(f\"ANOVA p-value = {anova_p:.4g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "mod = sm.QuantReg(df['resale_price'], sm.add_constant(df['storey_range']))\n",
    "for q in [0.1, 0.5, 0.9]:\n",
    "    res = mod.fit(q=q)\n",
    "    print(f\"q={q}: slope = {res.params[1]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is moderate collinearity between `storey_range` and `resale_price`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Feature Column: Floor Area (sqm) with HDB Resale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numerical_features_with_target(df, 'floor_area_sqm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['floor_area_sqm'].corr(df['resale_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is high collinearity between `floor_area_sqm` and the `resale_price`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pearson & Spearman in one go\n",
    "num_cols = df.select_dtypes('number').columns\n",
    "corrs = df[num_cols].corr(method='spearman')['resale_price'].sort_values(ascending=False)\n",
    "print(corrs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.regplot(x='floor_area_sqm', y='resale_price', data=df,\n",
    "            scatter_kws={'alpha':0.2}, line_kws={'color':'red'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bin X into deciles and plot median target\n",
    "df['area_bin'] = pd.qcut(df['floor_area_sqm'], 10, duplicates='drop')\n",
    "bin_stats = df.groupby('area_bin')['resale_price'].agg(['median','count'])\n",
    "bin_stats['median'].plot(marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60568f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dt = pd.to_datetime(df['month'])\n",
    "df_sorted = df.assign(month_dt=month_dt).sort_values('month_dt')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.lineplot(data=df_sorted, x='month_dt', y='resale_price', estimator='mean')\n",
    "\n",
    "ax.set_title('Average Resale Price')\n",
    "ax.set_xlabel('Year-Month')\n",
    "ax.set_ylabel('Resale Price')\n",
    "ax.xaxis.set_major_locator(ticker.LinearLocator(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0264a",
   "metadata": {},
   "source": [
    "**Average resale price varies over the years. This could be due to different flat types and location. It will be more meaningful to explore the average price by flat type. floor area and location.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9cf82",
   "metadata": {},
   "source": [
    "### Cyclical Analysis of Resale Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_counts = df.month.value_counts().sort_index()\n",
    "month_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.lineplot(x=month_counts.index, y=month_counts.values, ax=ax)\n",
    "\n",
    "ax.set_title('Trend of Number of Resale Listings Over the Months')\n",
    "ax.set_xlabel('Year-Month')\n",
    "ax.set_ylabel('Number of Resale Transaction')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b03e5",
   "metadata": {},
   "source": [
    "**HDB resale is seasonal and the month of February has the lowers resale transaction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Million Dollar HDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract resale price that is more than 1 million excluding 2016\n",
    "df_high_price = df[df['resale_price'] > 1000000].copy()\n",
    "\n",
    "# add year column \n",
    "df_high_price['year'] = pd.to_datetime(df_high_price['month']).dt.year\n",
    "df_high_price = df_high_price[df_high_price['year'] != 2019]\n",
    "\n",
    "# print the number of flats with resale price above 1 million\n",
    "print(f\"Number of flats with resale price above 1 million: {len(df_high_price)}\")\n",
    "\n",
    "# calculate the percentage of high resale price flats against total flats\n",
    "high_price_percentage = (len(df_high_price) / len(df)) * 100\n",
    "print(f\"Percentage of flats with resale price above 1 million: {high_price_percentage:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of resale price to see total number of high price flats for every year\n",
    "df_high_price['year'].value_counts().sort_index()\n",
    "df_high_price['year'].value_counts().sort_index().plot(kind='bar', figsize=(8,5))\n",
    "plt.title('Number of Flats with Resale Price Above 1 Million by Year excluding 2019')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Flats')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa211d68",
   "metadata": {},
   "source": [
    "### Town "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 3 town with the highest turn over is Jurong West, Woodlands and Sengkang.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349c099",
   "metadata": {},
   "source": [
    "### Flat Model Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56652175",
   "metadata": {},
   "source": [
    "**There are also missing values in flat model name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,10))  \n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sorted_flat_model_name = df.flatm_name.value_counts()\n",
    "sorted_flat_model_index = sorted_flat_model_name.index\n",
    "sns.countplot(data=df, x='flatm_name', order=sorted_flat_model_index, ax=ax1)\n",
    "\n",
    "ax1.set_title('Flats Count by Flat Model')\n",
    "ax1.set_xlabel('Flat Model')\n",
    "ax1.set_ylabel('Number of Flats Sold')\n",
    "ax1.set_xticks(ax1.get_xticks(), ax1.get_xticklabels(), rotation = 90)\n",
    "\n",
    "\n",
    "sorted_flat_model_id = df.flatm_id.value_counts()\n",
    "sorted_flat_model_id_index = sorted_flat_model_id.index\n",
    "sns.countplot(data=df, x='flatm_id', order=sorted_flat_model_id_index, ax=ax2)\n",
    "\n",
    "ax2.set_title('Flats Count by Flat Model ID')\n",
    "ax2.set_xlabel('Flat Model ID')\n",
    "ax2.set_ylabel('Number of Flats Sold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[sorted_flat_model_name.index, sorted_flat_model_id.index, sorted_flat_model_name, sorted_flat_model_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e292aa9",
   "metadata": {},
   "source": [
    "**Base on the table above, we can see that `Simplified`, `Apartment`, `Maisonette` and `Standard` flat model name can be filled by corresponding flat model id.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sorted_flat_model_name.index.to_list()\n",
    "model_id = sorted_flat_model_id.index.to_list()\n",
    "model_name_mapping = dict(zip(model_id, model_name))\n",
    "model_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.flatm_name.isnull(),'flatm_id'].map(model_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9601173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.flatm_name.isnull(), 'flatm_name'] = df.loc[df.flatm_name.isnull(),'flatm_id'].map(model_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19476687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 3 model with the most transaction is Model A, Improved and New Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff474003",
   "metadata": {},
   "source": [
    "### Flat Type and Resale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.boxplot(data=df, x='flat_type', y='resale_price', hue='flat_type', palette='Set1')\n",
    "\n",
    "ax.set_title('Resale Price by Flat Type')\n",
    "ax.set_xlabel('Flat Type')\n",
    "ax.set_ylabel('Resale Price')\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833be9ea",
   "metadata": {},
   "source": [
    "**There are many outliers for '2 ROOM', '3 ROOM', '4 ROOM', '5 ROOM', and 'EXECUTIVE' flats. Outliers likely happen maybe due to locations or renovation status.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832a221",
   "metadata": {},
   "source": [
    "### Flat Type and Floor Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.scatterplot(data=df, x='floor_area_sqm', y='resale_price', hue='flat_type', palette='Set1')\n",
    "\n",
    "ax.set_title('Scatter Plot of Floor Area vs Resale Price by Flat Type')\n",
    "ax.set_xlabel('Floor Area (sqm)')\n",
    "ax.set_ylabel('Resale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e585f",
   "metadata": {},
   "source": [
    "**Generally, the increase in floor area result in increase in resale price.**\n",
    "  \n",
    "**There are 3 ROOM flats that has a very high selling price and the floor area is also quite high. We need to investigate further.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8288af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_3_room = df[(df['flat_type'] == '3 ROOM') & (df['floor_area_sqm'] > 150) & (df['resale_price'] > 800000)]\n",
    "outliers_3_room"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6df902",
   "metadata": {},
   "source": [
    "**The outliers are 3 ROOM flats that are built in 1972 where they have floor area of 180 sqm and above. Therefore, flat type is not good enough to differentiate the resale price. Flat model also plays an important differentiator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrace_3_room = df[(df['flat_type'] == '3 ROOM') & (df['flatm_name'] == \"Terrace\")]\n",
    "terrace_3_room.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb982e2",
   "metadata": {},
   "source": [
    "- **Terrace was built between 1968 to 1972.**\n",
    "- **Both floor area and resale price and very wide range.**\n",
    "- **sqm range from 78 sqm to 280 sqm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da6afe",
   "metadata": {},
   "source": [
    "Checking other similar situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['flat_type', 'flatm_name', 'floor_area_sqm', 'resale_price']].groupby(['flat_type', 'flatm_name']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae7171",
   "metadata": {},
   "source": [
    "**Besides 3 ROOM flat, other flat type also contains Terrace model. To get a more accurate prediction, flat type and flat model must be used together.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Million Dollar HDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the distribution of resale price to see total number of high price flats for every year\n",
    "df_high_price['year'] = pd.to_datetime(df_high_price['month']).dt.year\n",
    "df_high_price['year'].value_counts().sort_index()\n",
    "df_high_price['year'].value_counts().sort_index().plot(kind='bar', figsize=(8,5))\n",
    "plt.title('Number of Flats with Resale Price Above 1 Million by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Flats')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53a3ea",
   "metadata": {},
   "source": [
    "### Average Resale Price By Town, Type and Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_breakdown = df[['town_name', 'floor_area_sqm', 'resale_price']].groupby(['town_name'])\n",
    "town_breakdown[['floor_area_sqm']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.boxplot(data=df, x='town_name', y='floor_area_sqm', hue='town_name', palette='Set1')\n",
    "\n",
    "ax.set_title('Floor Area by Town')\n",
    "ax.set_xlabel('Town')\n",
    "ax.set_ylabel('Floor Area (sqm)')\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels([label.get_text() for label in ax.get_xticklabels()], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The chart shows that some town contains more outliers than other town in terms of floor area. KALLANG/WHAMPOA has many outliers that goes beyond 250 sqm. These are the 3 Room terrace that we discovered earlier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_breakdown[['resale_price']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.boxplot(data=df, x='town_name', y='resale_price', hue='town_name', palette='Set1')\n",
    "\n",
    "ax.set_title('Resale Price by Town')\n",
    "ax.set_xlabel('Town')\n",
    "ax.set_ylabel('Resale Price')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some town command higher median selling price compared to other town.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "f1 = sns.histplot(data=df, x='town_name', hue='flat_type', palette='Set1', alpha=1)\n",
    "sns.move_legend(f1, \"upper left\", bbox_to_anchor=(1, 0.6))\n",
    "\n",
    "ax.set_title('Number of Transaction by Town and Flat Type')\n",
    "ax.set_xlabel('Town')\n",
    "ax.set_ylabel('Number of Transaction')\n",
    "ax.set_xticks(ax.get_xticks(), [label.get_text() for label in ax.get_xticklabels()], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some flat type such as 3 ROOM is only available for sales in older towns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "f1 = sns.histplot(data=df, x='town_name', hue='flatm_name', palette='Set2', alpha=1)\n",
    "sns.move_legend(f1, \"upper left\", bbox_to_anchor=(1, 0.6))\n",
    "\n",
    "ax.set_title('Number of Transaction by Town and Flat Model')\n",
    "ax.set_xlabel('Town')\n",
    "ax.set_ylabel('Number of Transaction')\n",
    "ax.set_xticks(ax.get_xticks(), [label.get_text() for label in ax.get_xticklabels()], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some model is only available at some town.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_type_breakdown = df[['town_name', 'flat_type', 'floor_area_sqm', 'resale_price']].groupby(['town_name', 'flat_type'])\n",
    "print(town_type_breakdown[['floor_area_sqm']].describe().to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(town_type_breakdown[['resale_price']].describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we breakdown the flat by town and flat type, the dispersion of the floor area and resale price is still wide. But the dispersion of floor area and resale price is narrower compared to just using one classification category. This is due to different flat model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_breakdown = df[['town_name', 'flat_type', 'flatm_name', 'floor_area_sqm', 'resale_price']].groupby(['town_name', 'flat_type', 'flatm_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_breakdown[['floor_area_sqm']].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_breakdown[['resale_price']].describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we breakdown the flat by town, flat type and flat model, for some location, there is no dispersion. However for some location, the dispersion of the floor area and resale price is much narrower compared to previous grouping and classification. However, some flat model still present a slightly wide dispersion in terms of flat model and resale price.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_flat_model_sqm = df[['flat_type', 'flatm_name', 'floor_area_sqm', 'resale_price']].groupby(['flat_type', 'flatm_name', 'floor_area_sqm']).mean()\n",
    "print(group_flat_model_sqm.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even if we fine tuned the flat selection by model type, there are many variation in terms of floor area. For example, in a 5 Room Model A flat we have a range of 129 sqm to 157 sqm. This proves that if we narrow our selection to model type, we may not have accurate prediction without the indication of floor area.**\n",
    "\n",
    "**Our initial conclusion is that the best feature to predict housing price is the floor area. However, we will keep the flat type and flat model to provide us segmentation information so that we can improve the prediction of resale price.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Break Down of Resale Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 ROOM with Model A by Town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_model_A = df.loc[(df['flat_type']=='4 ROOM') & (df['flatm_name']=='Model A'),['town_name', 'floor_area_sqm', 'resale_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.boxplot(data=four_model_A, x='town_name', y='resale_price', hue='town_name', palette='Set1')\n",
    "\n",
    "ax.set_title('Resale Price by Town for 4 Room Flat Model A')\n",
    "ax.set_xlabel('Town')\n",
    "ax.set_ylabel('Resale Price')\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels([label.get_text() for label in ax.get_xticklabels()], rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The more we breakdown by town, flat type and flat model, the less dispersion is the resale price. However, it is clear that not every town command the same resale price for the same type of flat with the same model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining Lease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.remaining_lease.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remaining lease is either in year or in year and month. We assume numbers without indicating month or year belongs to year.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.remaining_lease.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to convert the remaining lease to months as a standard indicator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lease_to_month(lease):\n",
    "    \"\"\"\n",
    "    Convert remaining lease period from string to total number of months.\n",
    "    Args:\n",
    "        remaining_lease in (str)\n",
    "\n",
    "    Returns: \n",
    "        integer\n",
    "\n",
    "    Example:\n",
    "        convert_lease_to_month('07 TO 09') -> 8.0  \n",
    "    \"\"\"\n",
    "    str_list = lease.split(' ')\n",
    "    if ('months' in str_list) | ('month' in str_list):\n",
    "        year = int(str_list[0])\n",
    "        month = int(str_list[2])\n",
    "        t_month = (year * 12) + month \n",
    "    elif ('years' in str_list) & (('months' not in str_list) | ('month' not in str_list)):\n",
    "        year = int(str_list[0])\n",
    "        t_month = (year * 12)\n",
    "    else:\n",
    "        year = int(str_list[0])\n",
    "        t_month = (year * 12)        \n",
    "    return t_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['remaining_lease_by_month'] = df.remaining_lease.apply(convert_lease_to_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c1136f",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50223bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_column = df[['floor_area_sqm', 'lease_commence_date', 'remaining_lease_by_month', 'resale_price']]\n",
    "corr_matrix_pear = corr_column.corr(method='pearson')\n",
    "corr_matrix_pear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.heatmap(corr_matrix_pear, annot=True, cmap='coolwarm', vmin=0, center=0.5, vmax=1, linewidths=0.5, fmt=\".2f\")\n",
    "\n",
    "ax.set_title('Pearson Correlation Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f8fc7",
   "metadata": {},
   "source": [
    "**There is a strong correlation between floor area and resale price. There is also moderate correlation between resale price and lease commence date which in turn indicating the age of the flat. The features, lease commence date and remaining lease in months are highly correlated. Therefore, it is suggested that we either use Ridge Regression to reduce the impact of multicollinearity or we drop one of the feature.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46eee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.storey_range = df.storey_range.apply(convert_storey_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.storey_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_col = df[['storey_range', 'floor_area_sqm', 'lease_commence_date', 'remaining_lease_by_month', 'resale_price']]\n",
    "spearman_corr = spearman_col.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25585deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', vmin=0, center=0, vmax=1, linewidths=0.5, fmt=\".2f\")\n",
    "\n",
    "ax.set_title('Spearman Correlation Matrix Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d5008",
   "metadata": {},
   "source": [
    "**There is strong correlation between floor area and resale price. Moderate correlation between age of the flat and the resale price. Moderate to weak correlation between height of the flat and resale price. Similarly, there is high correlation between lease commence date and remaining lease in months. Remaining lease in months are slightly more correlated to the resale price, as it is more granular. Therefore, we can consider dropping lease commence date if necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))  \n",
    "\n",
    "sns.lineplot(df, x = df.floor_area_sqm, y=df.resale_price)\n",
    "\n",
    "ax.set_title('Price vs Floor Area (sqm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a linear trend in terms of resale price and its biggest contributor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resale Price**\n",
    "- **The mean of resale price is around $438K and the median is around 408K. Resale prices range from $160K to $1.2M. Based on the chart, the distribution of the resale price skewed to the right. This means that there are expensive flat that extends to the right. Lower priced flats are more concentrated. The skewness will cause the model to over-estimate the normal range houses. Either we use transformation on the target or we use other model that are robust to the skewness.**\n",
    "\n",
    "**Feature Analysis**\n",
    "- **Top 3 town with the highest turn over is Jurong West, Woodlands and Sengkang. The most popular resale flat type is 4 ROOM, follow by 3 ROOM and 5 ROOM flat. Top 3 model with the most transaction is Model A, Improved and New Generation.**\n",
    "- **For 3 ROOM flats, there are outliers. These outliers belongs to old 3 ROOM flat that comes with a Terrace model. These flats are built between 1968 and 1972. The floor area ranges from 78 sqm to 880 sqm. Therefore, flat type is not good enough to differentiate the resale price. Flat model also plays an important differentiator.**\n",
    "- **Besides 3 ROOM flat, other flat type also contains Terrace model. To get a more accurate prediction, flat type and flat model must be used together.**\n",
    "- **Some flat type such as 3 ROOM is only available at older towns. Some model such as 'New Generation' is only available at some town.**\n",
    "- **For the same flat type with the same model and similar floor area, not every town command the similar resale price range. Some town has higher median resale price compared to other town. This proves that location matters.**\n",
    "- **If we breakdown the flat by town, flat type and flat model, for some location, there is no dispersion. However for some location, the dispersion of the floor area and resale price is much narrower compared to fewer grouping. However, some flat model still present a slightly wide dispersion in terms of flat model and resale price.**\n",
    "- **Even if we fine tuned the flat selection by model type, there are many variation in terms of floor area. For example, in a 5 Room Model A flat we have a range of floor area between 129 sqm to 157 sqm. This proves that if we narrow our selection to flat type and model type, we may not have accurate prediction without the indication of floor area.**\n",
    "\n",
    "**Feature Analysis Conclusion** \n",
    "- **Our initial conclusion is that the best feature to predict housing price is the floor area. However, we will can keep the flat type and flat model to provide us segmentation information so that we can improve the prediction of resale price. Location information is important to differentiate the price in different town.**\n",
    "\n",
    "**Correlation Analysis**\n",
    "- **For Pearson correlation analysis, there is a strong correlation between floor area and resale price. There is also moderate correlation between resale price and lease commence date which in turn indicating the age of the flat. The features, lease commence date and remaining lease in months are highly correlated. Therefore, it is suggested that we either use Ridge Regression to reduce the impact of multicollinearity or we drop one of the feature.** \n",
    "- **For Spearman Correlation Analysis, there is strong correlation between floor area and resale price. Moderate correlation between age of the flat and the resale price. Moderate to weak correlation between height of the flat and resale price. Similarly, there is high correlation between lease commence date and remaining lease in months. Remaining lease in months are slightly more correlated to the resale price, as it is more granular. Therefore, we can consider dropping lease commence date if necessary.**\n",
    "- **Additional chart show a linear relation between hdb resale price and the floor area.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Floor area is the most important feature follow by age of the flat and the height of the flat.**\n",
    "- **For the age of the flat, we will drop the column `lease_commence_date` and keep `remaining_lease_by_month`.** \n",
    "- **Other factor that are not in correlation analysis such as location information (`town_name`) also provide important signal on pricing prediction.**\n",
    "- **We will keep the `flat_type` and `flatm_name` (flat model) to improve the prediction by providing segmental information.**\n",
    "- **Investigation in our EDA suggested that there are many variation in `flat_type`. For example, a 3-room flat, the floor area ranges from 78sqm to 280 sqm. Therefore, we think that `flat_type` should be nominal instead of ordinal.**\n",
    "- **Lastly, we will keep transaction year and month, to help with the seasonal resale price prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ada890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./data/resale_transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated items\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Negatives Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negative number in lease commence date\n",
    "df.lease_commence_date = df.lease_commence_date.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Categorical Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.flat_type = df.flat_type.replace('FOUR ROOM', '4 ROOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.flat_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_missing_name(df: pd.DataFrame, missing_name_col: str, missing_name_related_id_col:str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Fills missing values in the 'name' column from the 'id' column.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        df (pd.DataFrame): The DataFrame containing the columns to be fixed.\n",
    "        missing_name_related_id_col (str): The name of the column containing the IDs that matches the name.\n",
    "        missing_name_col (str): The name of the column containing missing names to be filled.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame: The DataFrame with missing values fixed.\n",
    "    \"\"\"\n",
    "\n",
    "    missing_name_rows = df[missing_name_col].isnull()\n",
    "    list_name = df[missing_name_col].value_counts().index.to_list()\n",
    "    list_id = df[missing_name_related_id_col].value_counts().index.to_list()\n",
    "\n",
    "    missing_name_mapping = dict(zip(list_id, list_name))\n",
    "    #print(missing_name_mapping)\n",
    "    \n",
    "    df.loc[missing_name_rows, missing_name_col] = df.loc[missing_name_rows, missing_name_related_id_col].map(missing_name_mapping)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'town_name' column\n",
    "df = handling_missing_name(df, missing_name_col='town_name', missing_name_related_id_col='town_id')\n",
    "\n",
    "# Fill missing values in 'flatm_name' column\n",
    "df = handling_missing_name(df=df, missing_name_related_id_col='flatm_id', missing_name_col='flatm_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Splitting Transaction Column 'month' to Year and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting column 'month' into year and month\n",
    "df['year_month'] = pd.to_datetime(df.month, format='%Y-%m')\n",
    "df['transac_year'] = df.year_month.dt.year\n",
    "df['transac_month'] = df.year_month.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Convert 'remaining_lease' to remaining_lease_months'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lease_to_month(lease: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert remaining lease period from string to total number of months.\n",
    "    Args:\n",
    "        remaining_lease in (str)\n",
    "\n",
    "    Returns: \n",
    "        integer\n",
    "\n",
    "    Example:\n",
    "        convert_lease_to_month('07 TO 09') -> 8.0  \n",
    "    \"\"\"\n",
    "    str_list = lease.split(' ')\n",
    "    if ('months' in str_list) | ('month' in str_list):\n",
    "        year = int(str_list[0])\n",
    "        month = int(str_list[2])\n",
    "        total_month = (year * 12) + month \n",
    "    elif ('years' in str_list) & (('months' not in str_list) | ('month' not in str_list)):\n",
    "        year = int(str_list[0])\n",
    "        total_month = (year * 12)\n",
    "    else:\n",
    "        year = int(str_list[0])\n",
    "        total_month = (year * 12)        \n",
    "    return total_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column remaining lease to remaining lease by  months\n",
    "df['remaining_lease_by_months'] = df.remaining_lease.apply(convert_lease_to_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['remaining_lease', 'remaining_lease_by_months']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - 'storey_range'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_storey_range(storey_range: str) -> float:\n",
    "    \"\"\"\n",
    "    Convert storey range to the numerical average.\n",
    "    Args:\n",
    "        storey_range in (str)\n",
    "\n",
    "    Returns: \n",
    "        float\n",
    "\n",
    "    Example:\n",
    "        convert_storey_range('07 TO 09') -> 8.0 \n",
    "    \"\"\"\n",
    "\n",
    "    low, high = storey_range.split(' TO ')\n",
    "    average = (int(low) + int(high)) / 2\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert storey range to number middle value\n",
    "df.storey_range = df.storey_range.apply(convert_storey_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns for machine learning preparation\n",
    "irrelevant_columns = ['id', 'month', 'block', 'street_name', 'remaining_lease', 'town_id',  'flatm_id', 'year_month', 'lease_commence_date']\n",
    "df.drop(columns = irrelevant_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into features and target\n",
    "X = df.drop(columns='resale_price')\n",
    "y = df['resale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (80%) and test-validation (20%) sets\n",
    "X_train, X_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test-validation set (20%) into validation (10%) and test (10%) sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_validation_and_test, y_validation_and_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shapes of the splits to verify\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that are numerical for feature scaling preparation\n",
    "numerical_features = ['floor_area_sqm', 'remaining_lease_by_months', 'transac_year'] # no lease_commence_date\n",
    "\n",
    "degree = 1  # Degree of polynomial features, can be adjusted\n",
    "# Create a numerical transformer pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('polynomial_features', PolynomialFeatures(degree=degree)),  # Placeholder for polynomial features\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that need to be one-hot encoded\n",
    "nominal_features = ['transac_month', 'town_name', 'flatm_name', 'flat_type']\n",
    "\n",
    "# Select columns that requires ordinal encoding\n",
    "ordinal_features = []\n",
    "\n",
    "# Set the criterial for ordinal encoding\n",
    "# flat_type_categories = ['1 ROOM', '2 ROOM', '3 ROOM', '4 ROOM', '5 ROOM', 'MULTI-GENERATION', 'EXECUTIVE']\n",
    "\n",
    "# Select the columns that do not required further processing \n",
    "passthrough_features = ['storey_range']\n",
    "\n",
    "# Setting pipeline for one-hot encoding\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Setting pipeline for ordinal encoding\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('ordinal', OrdinalEncoder(categories=[flat_type_categories], handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', 'passthrough')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1  # Degree of polynomial features, can be adjusted\n",
    "# Create a numerical transformer pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('polynomial_features', PolynomialFeatures(degree=degree)),  # Placeholder for polynomial features\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pipeline for one-hot encoding\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pipeline for ordinal encoding\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('ordinal', OrdinalEncoder(categories=[flat_type_categories], handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', 'passthrough')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_features),\n",
    "        ('ord', 'passthrough', ordinal_features),\n",
    "        ('pass', 'passthrough', passthrough_features) \n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression without Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up regression pipeline\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "lr_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on validation set\n",
    "y_val_pred = lr_pipeline.predict(X_val)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "feature_names = lr_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model coefficients\n",
    "model_coefficients = lr_pipeline.named_steps['regressor'].coef_\n",
    "print(model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set\n",
    "lr_val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "lr_val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "lr_val_rmse = root_mean_squared_error(y_val, y_val_pred)  \n",
    "lr_val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display the metrics\n",
    "print('Linear Regression Performance Metrics:')\n",
    "print(f\"Linear Regression Validation MAE: {lr_val_mae}\")\n",
    "print(f\"Linear Regression Validation MSE: {lr_val_mse}\")\n",
    "print(f\"Linear Regression Validation RMSE: {lr_val_rmse}\")\n",
    "print(f\"Linear Regression Validation R2: {lr_val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_val - y_val_pred[0]\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual plot shows that the model did not capture non-linearity in the target. Residual distribution remains skewed. Will try target transformation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression with Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "y_train_transformed = pt.fit_transform(y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the transformed target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_train_transformed, kde=True, bins=30)\n",
    "plt.title('Transformed Target Variable Distribution')\n",
    "plt.xlabel('Transformed Resale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "lr_pipeline.fit(X_train, y_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on validation set\n",
    "y_val_pred_transformed = lr_pipeline.predict(X_val)\n",
    "#print(type(y_val_pred_transformed))\n",
    "#print(y_val_pred_transformed.shape)\n",
    "y_val_pred = pt.inverse_transform(y_val_pred_transformed.reshape(-1, 1))  # type: ignore # Inverse transform to get original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set\n",
    "lr_val_mae_transformed = mean_absolute_error(y_val, y_val_pred)\n",
    "lr_val_mse_transformed = mean_squared_error(y_val, y_val_pred)\n",
    "lr_val_rmse_transformed = root_mean_squared_error(y_val, y_val_pred)  \n",
    "lr_val_r2_transformed = r2_score(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Validation MAE (Transformed): {lr_val_mae_transformed}\")\n",
    "print(f\"Validation MSE (Transformed): {lr_val_mse_transformed}\")\n",
    "print(f\"Validation RMSE (Transformed): {lr_val_rmse_transformed}\")\n",
    "print(f\"Validation R2 (Transformed): {lr_val_r2_transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_val - y_val_pred[0]\n",
    "\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Predicted Values (Transformed)')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.title('Residuals Distribution (Transformed)')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual plot in transform space\n",
    "y_pred_t = lr_pipeline.predict(X_val)          \n",
    "if isinstance(y_pred_t, tuple):\n",
    "\ty_pred_t_flat = y_pred_t[0].flatten()\n",
    "else:\n",
    "\ty_pred_t_flat = y_pred_t.flatten()\n",
    "resid_t = y_train_transformed.flatten()[:len(y_pred_t_flat)] - y_pred_t_flat\n",
    "plt.scatter(y_pred_t_flat, resid_t, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Residuals vs Predicted (Transformed Space)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the residual plot after target transformation, we can see that linear regression could not catch non-linearity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the metrics\n",
    "print(f\"Validation MAE: {lr_val_mae}\")\n",
    "print(f\"Validation MSE: {lr_val_mse}\")\n",
    "print(f\"Validation RMSE: {lr_val_rmse}\")\n",
    "print(f\"Validation R2: {lr_val_r2}\")\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Validation MAE (Transformed): {lr_val_mae_transformed}\")\n",
    "print(f\"Validation MSE (Transformed): {lr_val_mse_transformed}\")\n",
    "print(f\"Validation RMSE (Transformed): {lr_val_rmse_transformed}\")\n",
    "print(f\"Validation R2 (Transformed): {lr_val_r2_transformed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation likely helped normalize the error distribution and reduce the impact of large errors (outliers) in the transformed space. This makes the model more accurate on average, leading to a better Mean Absolute Error.\n",
    "\n",
    "RMSE got worst and there is a slight dip in R-squared due to the inverse transformation. While the model might be performing well in the transformed scale, any prediction errors, particularly those on the higher end of the original data, get magnified when inverse transformed back to the original scale. Since RMSE heavily penalizes these larger errors (by squaring them), it increases. This overall increase in magnified errors also leads to a slight decrease in R-squared, as the model explains less of the variance in the original scale.\n",
    "\n",
    "Essentially, the transformation optimized for model performance in a normalized space, but the re-scaling back to the original units amplified certain errors, negatively impacting RMSE and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformed target did decrease MAE, which is our business objective, residual plot shows that the model failed to capture non-linearity. We will test with Polynomial regression, Ridge and Lasso regression. Will also try Huber Regression and Quantile Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression with/without Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 6  # Degree of polynomial features, can be adjusted\n",
    "# Create a numerical transformer pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('polynomial_features', PolynomialFeatures(degree=degree)),  # Placeholder for polynomial features\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_features),\n",
    "        ('ord', 'passthrough', ordinal_features),\n",
    "        ('pass', 'passthrough', passthrough_features) \n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up regression pipeline\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression (No Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = lr_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set\n",
    "poly_val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "poly_val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "poly_val_rmse = root_mean_squared_error(y_val, y_val_pred)  \n",
    "poly_val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Validation MAE: {poly_val_mae}\")\n",
    "print(f\"Validation MSE: {poly_val_mse}\")\n",
    "print(f\"Validation RMSE: {poly_val_rmse}\")\n",
    "print(f\"Validation R2: {poly_val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression (Transformed Target Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformedTargetRegressor_model = TransformedTargetRegressor(\n",
    "    regressor    = lr_pipeline,\n",
    "    transformer  = PowerTransformer(method='yeo-johnson')\n",
    ")\n",
    "TransformedTargetRegressor_model.fit(X_train, y_train)\n",
    "y_val_pred = TransformedTargetRegressor_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set\n",
    "poly_val_mae_transformed = mean_absolute_error(y_val, y_val_pred)\n",
    "poly_val_mse_transformed = mean_squared_error(y_val, y_val_pred)\n",
    "poly_val_rmse_transformed = root_mean_squared_error(y_val, y_val_pred)  \n",
    "poly_val_r2_transformed = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Polynomial Regression Validation MAE with {degree} polynomial degree: {poly_val_mae}\")\n",
    "print(f\"Polynomial Regression Validation MSE with {degree} polynomial degree: {poly_val_mse}\")\n",
    "print(f\"Polynomial Regression Validation RMSE with {degree} polynomial degree: {poly_val_rmse}\")\n",
    "print(f\"Polynomial Regression Validation R2 with {degree} polynomial degree: {poly_val_r2}\")\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Polynomial Regression Validation MAE (Transformed)  with {degree} polynomial degree: {poly_val_mae_transformed}\")\n",
    "print(f\"Polynomial Regression Validation MSE (Transformed)  with {degree} polynomial degree: {poly_val_mse_transformed}\")\n",
    "print(f\"Polynomial Regression Validation RMSE (Transformed)  with {degree} polynomial degree: {poly_val_rmse_transformed}\")\n",
    "print(f\"Polynomial Regression Validation R2 (Transformed)  with {degree} polynomial degree: {poly_val_r2_transformed}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results shows that target transformation in polynomial regression did improve the performance slightly.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pipeline for Ridge Regression \n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "y_val_pred_ridge = ridge_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set with Ridge Regression\n",
    "val_mae_ridge = mean_absolute_error(y_val, y_val_pred_ridge)\n",
    "val_mse_ridge = mean_squared_error(y_val, y_val_pred_ridge)\n",
    "val_rmse_ridge = root_mean_squared_error(y_val, y_val_pred_ridge)  \n",
    "val_r2_ridge = r2_score(y_val, y_val_pred_ridge)\n",
    "\n",
    "# Print the metrics for Ridge Regression\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} Validation MAE: {val_mae_ridge}\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} MSE: {val_mse_ridge}\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} RMSE: {val_rmse_ridge}\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} R²: {val_r2_ridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Transformation for Ridge \n",
    "TransformedTargetRegressor_model = TransformedTargetRegressor(\n",
    "    regressor    = ridge_pipeline,\n",
    "    transformer  = PowerTransformer(method='yeo-johnson')\n",
    ")\n",
    "TransformedTargetRegressor_model.fit(X_train, y_train)\n",
    "y_val_pred_ridge_transformed = TransformedTargetRegressor_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate regression metrics for validation set with Ridge Regression\n",
    "val_mae_ridge_transformed = mean_absolute_error(y_val, y_val_pred_ridge_transformed)\n",
    "val_mse_ridge_transformed = mean_squared_error(y_val, y_val_pred_ridge_transformed)\n",
    "val_rmse_ridge_transformed = root_mean_squared_error(y_val, y_val_pred_ridge_transformed)  \n",
    "val_r2_ridge_transformed = r2_score(y_val, y_val_pred_ridge_transformed)\n",
    "\n",
    "# Print the metrics for Ridge Regression\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} Target Transformation Validation MAE: {val_mae_ridge_transformed}\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} Target Transformation MSE: {val_mse_ridge_transformed}\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} Target Transformation RMSE: {val_rmse_ridge_transformed}\")\n",
    "print(f\"Ridge Regression polynomial degree:{degree} Target Transformation R²: {val_r2_ridge_transformed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "\n",
    "# Fit Lasso Regression with default alpha\n",
    "lasso_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso(alpha = 0.001, max_iter=3000, random_state=42))\n",
    "])\n",
    "\n",
    "TransformedTargetRegressor_model = TransformedTargetRegressor(\n",
    "    regressor    = lasso_pipeline,\n",
    "    transformer  = PowerTransformer(method='yeo-johnson')\n",
    ")\n",
    "TransformedTargetRegressor_model.fit(X_train, y_train)\n",
    "y_val_pred_lasso = TransformedTargetRegressor_model.predict(X_val)\n",
    "\n",
    "# Calculate regression metrics for validation set with Lasso Regression\n",
    "val_mae_lasso_transformed = mean_absolute_error(y_val, y_val_pred_lasso)\n",
    "val_mse_lasso_transformed = mean_squared_error(y_val, y_val_pred_lasso)\n",
    "val_rmse_lasso_transformed = root_mean_squared_error(y_val, y_val_pred_lasso)  # RMSE is the square root of MSE\n",
    "val_r2_lasso_transformed = r2_score(y_val, y_val_pred_lasso)\n",
    "\n",
    "# Display the metrics for Lasso Regression\n",
    "print(\"Lasso Regression Metrics:\")\n",
    "print(f\"Lasso Transformed Validation MAE: {val_mae_lasso_transformed}\")\n",
    "print(f\"Lasso Transformed Validation MSE: {val_mse_lasso_transformed}\")\n",
    "print(f\"Lasso Transformed Validation RMSE: {val_rmse_lasso_transformed}\")\n",
    "print(f\"Lasso Transformed Validation R²: {val_r2_lasso_transformed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "\n",
    "# Fit Lasso Regression with default alpha\n",
    "lasso_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "y_val_pred_lasso = lasso_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate regression metrics for validation set with Lasso Regression\n",
    "val_mae_lasso = mean_absolute_error(y_val, y_val_pred_lasso)\n",
    "val_mse_lasso = mean_squared_error(y_val, y_val_pred_lasso)\n",
    "val_rmse_lasso = root_mean_squared_error(y_val, y_val_pred_lasso)  # RMSE is the square root of MSE\n",
    "val_r2_lasso = r2_score(y_val, y_val_pred_lasso)\n",
    "\n",
    "# Display the metrics for Lasso Regression\n",
    "print(\"Lasso Regression Metrics:\")\n",
    "print(f\"Lasso Validation MAE: {val_mae_lasso}\")\n",
    "print(f\"Lasso Validation MSE: {val_mse_lasso}\")\n",
    "print(f\"Lasso Validation RMSE: {val_rmse_lasso}\")\n",
    "print(f\"Lasso Validation R²: {val_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an experiment on Huber Regression as no transformation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 6  # Degree of polynomial features, can be adjusted\n",
    "# Create a numerical transformer pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('polynomial_features', PolynomialFeatures(degree=degree)),  # Placeholder for polynomial features\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_features),\n",
    "        ('ord', 'passthrough', ordinal_features),\n",
    "        ('pass', 'passthrough', passthrough_features) \n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pipeline for Huber Regression \n",
    "huber_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', HuberRegressor(epsilon=1.1, max_iter=3000, alpha=0.0)) # usually 1.35 epsilon\n",
    "])\n",
    "\n",
    "\n",
    "huber_pipeline.fit(X_train, y_train)\n",
    "y_val_pred_huber = huber_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set with Huber Regression\n",
    "val_mae_huber = mean_absolute_error(y_val, y_val_pred_huber)\n",
    "val_mse_huber = mean_squared_error(y_val, y_val_pred_huber)\n",
    "val_rmse_huber = root_mean_squared_error(y_val, y_val_pred_huber)  \n",
    "val_r2_huber = r2_score(y_val, y_val_pred_huber)\n",
    "\n",
    "# Display the metrics for Huber Regression\n",
    "print(\"Huber Regression Metrics:\")\n",
    "print(f\"Huber Validation MAE: {val_mae_huber}\")\n",
    "print(f\"Huber Validation MSE: {val_mse_huber}\")\n",
    "print(f\"Huber Validation RMSE: {val_rmse_huber}\")\n",
    "print(f\"Huber Validation R²: {val_r2_huber}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an experiment on Quantile regression to check the performance against traditional linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pipeline for Quantile Regression (mid quantile)\n",
    "quantile_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', QuantileRegressor(quantile=0.5, alpha=1e-3, solver='highs-ds'))\n",
    "])\n",
    "\n",
    "quantile_pipeline.fit(X_train, y_train)\n",
    "y_val_pred_quant_5 = quantile_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set with Quantile Regression\n",
    "val_mae_quant_5 = mean_absolute_error(y_val, y_val_pred_quant_5)\n",
    "val_mse_quant_5 = mean_squared_error(y_val, y_val_pred_quant_5)\n",
    "val_rmse_quant_5 = root_mean_squared_error(y_val, y_val_pred_quant_5)  \n",
    "val_r2_quant_5 = r2_score(y_val, y_val_pred_quant_5)\n",
    "\n",
    "# Display the metrics for Quantile Regression\n",
    "print(\"Quantile Regression Metrics:\")\n",
    "print(f\"Quantile Validation MAE: {val_mae_quant_5}\")\n",
    "print(f\"Quantile Validation MSE: {val_mse_quant_5}\")\n",
    "print(f\"Quantile Validation RMSE: {val_rmse_quant_5}\")\n",
    "print(f\"Quantile Validation R²: {val_r2_quant_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pipeline for Quantile Regression (top end quantile)\n",
    "quantile_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', QuantileRegressor(quantile=0.9, alpha=1e-3, solver='highs-ds'))\n",
    "])\n",
    "\n",
    "quantile_pipeline.fit(X_train, y_train)\n",
    "y_val_pred_quant_9 = quantile_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for validation set with Quantile Regression\n",
    "val_mae_quant_9 = mean_absolute_error(y_val, y_val_pred_quant_9)\n",
    "val_mse_quant_9 = mean_squared_error(y_val, y_val_pred_quant_9)\n",
    "val_rmse_quant_9 = root_mean_squared_error(y_val, y_val_pred_quant_9)  \n",
    "val_r2_quant_9 = r2_score(y_val, y_val_pred_quant_9)\n",
    "\n",
    "# Display the metrics for Quantile Regression\n",
    "print(\"Quantile Regression Metrics:\")\n",
    "print(f\"Quantile Validation MAE: {val_mae_quant_9}\")\n",
    "print(f\"Quantile Validation MSE: {val_mse_quant_9}\")\n",
    "print(f\"Quantile Validation RMSE: {val_rmse_quant_9}\")\n",
    "print(f\"Quantile Validation R²: {val_r2_quant_9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantile and Huber Regression did not outperform traditional linear model. We will compare metric on Polynomial, Ridge and Lasso Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model Metric Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics to dictionary for each model with category\n",
    "metrics_poly = {\n",
    "    \"Category\": \"Original\",\n",
    "    \"Model\": \"Polynomial Regression\",\n",
    "    \"MAE\":  poly_val_mae,\n",
    "    \"RMSE\": poly_val_rmse,\n",
    "    \"R2\":   poly_val_r2,\n",
    "}\n",
    "\n",
    "metrics_poly_transformed = {\n",
    "    \"Category\": \"Target Transformed\",\n",
    "    \"Model\": \"Polynomial Regression\",\n",
    "    \"MAE\":  poly_val_mae_transformed,\n",
    "    \"RMSE\": poly_val_rmse_transformed,\n",
    "    \"R2\":   poly_val_r2_transformed,\n",
    "}\n",
    "\n",
    "metrics_ridge = {\n",
    "    \"Category\": \"Original\",\n",
    "    \"Model\": \"Ridge Regression\",\n",
    "    \"MAE\":  val_mae_ridge,\n",
    "    \"RMSE\": val_rmse_ridge,\n",
    "    \"R2\":   val_r2_ridge,\n",
    "}\n",
    "\n",
    "metrics_ridge_transformed = {\n",
    "    \"Category\": \"Target Transformed\",\n",
    "    \"Model\": \"Ridge Regression\",\n",
    "    \"MAE\":  val_mae_ridge_transformed,\n",
    "    \"RMSE\": val_rmse_ridge_transformed,\n",
    "    \"R2\":   val_r2_ridge_transformed,\n",
    "}\n",
    "\n",
    "metrics_lasso = {\n",
    "    \"Category\": \"Original\",\n",
    "    \"Model\": \"Lasso Regression\",\n",
    "    \"MAE\":  val_mae_lasso,\n",
    "    \"RMSE\": val_rmse_lasso,\n",
    "    \"R2\":   val_r2_lasso,\n",
    "}\n",
    "\n",
    "metrics_lasso_transformed = {\n",
    "    \"Category\": \"Target Transformed\",\n",
    "    \"Model\": \"Lasso Regression\",\n",
    "    \"MAE\":  val_mae_lasso_transformed,\n",
    "    \"RMSE\": val_rmse_lasso_transformed,\n",
    "    \"R2\":   val_r2_lasso_transformed,\n",
    "}\n",
    "\n",
    "# Append all metrics dictionaries to the results list\n",
    "all_results = [\n",
    "    metrics_poly,\n",
    "    metrics_ridge,\n",
    "    metrics_lasso,\n",
    "    metrics_poly_transformed,\n",
    "    metrics_ridge_transformed,\n",
    "    metrics_lasso_transformed\n",
    "]\n",
    "\n",
    "# Create DataFrame with multi-level index\n",
    "results_df = (\n",
    "    pd.DataFrame(all_results)\n",
    "      .set_index([\"Category\", \"Model\"])\n",
    "      .round(4)\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Alternative: Create separate sections with cleaner display\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE - SEPARATED BY TRANSFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group by category and display each section\n",
    "for category in results_df.index.get_level_values('Category').unique():\n",
    "    print(f\"\\n{category.upper()} MODELS:\")\n",
    "    print(\"-\" * 80)\n",
    "    section_df = results_df.loc[category].round(4)\n",
    "    print(section_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization will reduce validation performance, but the difference is not that huge if we are looking at R2 or MAE. We need further fine tuning and apply the model to the test set before we can decide who model to use. However, we will try Decision Tree model since our target are non-linear.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Based Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('nom', nominal_transformer, nominal_features),\n",
    "        ('ord', 'passthrough', ordinal_features),\n",
    "        ('pass', 'passthrough', passthrough_features) \n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "dt_pipeline.fit(X_train, y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set with Decision Tree Regressor\n",
    "y_val_pred_dt = dt_pipeline.predict(X_val)  \n",
    "# Calculate regression metrics for validation set with Decision Tree Regressor\n",
    "val_mae_dt = mean_absolute_error(y_val, y_val_pred_dt)\n",
    "val_mse_dt = mean_squared_error(y_val, y_val_pred_dt)\n",
    "val_rmse_dt = root_mean_squared_error(y_val, y_val_pred_dt)\n",
    "val_r2_dt = r2_score(y_val, y_val_pred_dt)  \n",
    "\n",
    "# Display the metrics for Decision Tree Regressor\n",
    "print(\"Decision Tree Regressor Metrics:\")\n",
    "print(f\"Decision Tree Validation MAE: {val_mae_dt}\")\n",
    "print(f\"Decision Tree Validation MSE: {val_mse_dt}\")\n",
    "print(f\"Decision Tree Validation RMSE: {val_rmse_dt}\")\n",
    "print(f\"Decision Tree Validation R²: {val_r2_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature importance\n",
    "dt_feature_importances = dt_pipeline.named_steps['regressor'].feature_importances_\n",
    "dt_feature_names = preprocessor.get_feature_names_out()\n",
    "dt_feature_importances_df = pd.DataFrame({'Feature': dt_feature_names, 'Importance': dt_feature_importances})\n",
    "dt_feature_importances_df = dt_feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"\\nDecision Tree Feature Importances (Top 20):\")\n",
    "print(dt_feature_importances_df[:20])  # Display top 20 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using basic decision tree model without any parameter tuning, our error reduced and performance improve is much better than Linear Regression with transformed target. We have a MAE of $30K and R-squared of 90%. We believe that any parameter fine tuning on linear model will not be able to outperform the decision tree base line model. We will stop using linear models and adopt tree-based model as our recommended model.**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the validation set with Random Forest Regressor\n",
    "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate regression metrics for validation set with Random Forest Regressor\n",
    "val_mae_rf = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "val_mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "val_rmse_rf = root_mean_squared_error(y_val, y_val_pred_rf)\n",
    "val_r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Display the metrics for Random Forest Regressor\n",
    "print(\"Random Forest Regressor Metrics:\")\n",
    "print(f\"Random Forest Validation MAE: {val_mae_rf}\")\n",
    "print(f\"Random Forest Validation MSE: {val_mse_rf}\")\n",
    "print(f\"Random Forest Validation RMSE: {val_rmse_rf}\")\n",
    "print(f\"Random Forest Validation R²: {val_r2_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature importances for Random Forest\n",
    "rf_feature_importances = rf_pipeline.named_steps['regressor'].feature_importances_\n",
    "rf_feature_names = preprocessor.get_feature_names_out()\n",
    "rf_feature_importances_df = pd.DataFrame({'Feature': rf_feature_names, 'Importance': rf_feature_importances})\n",
    "rf_feature_importances_df = rf_feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "# Print the feature importances for Random Forest\n",
    "print(\"\\nRandom Forest Feature Importances (Top 20):\")\n",
    "print(rf_feature_importances_df[:20])  # Display top 20 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(random_state=42, n_jobs=-1, verbosity=0))\n",
    "])\n",
    "xgb_pipeline.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the validation set with XGBoost Regressor\n",
    "y_val_pred_xgb = xgb_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate regression metrics for validation set with XGBoost Regressor\n",
    "val_mae_xgb = mean_absolute_error(y_val, y_val_pred_xgb)\n",
    "val_mse_xgb = mean_squared_error(y_val, y_val_pred_xgb)\n",
    "val_rmse_xgb = root_mean_squared_error(y_val, y_val_pred_xgb)\n",
    "val_r2_xgb = r2_score(y_val, y_val_pred_xgb)\n",
    "\n",
    "# Display the metrics for XGBoost Regressor\n",
    "print(\"XGBoost Regressor Metrics:\")\n",
    "print(f\"XGBoost Validation MAE: {val_mae_xgb}\")\n",
    "print(f\"XGBoost Validation MSE: {val_mse_xgb}\")\n",
    "print(f\"XGBoost Validation RMSE: {val_rmse_xgb}\")\n",
    "print(f\"XGBoost Validation R²: {val_r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature importances of XGBoost\n",
    "xgb_feature_importances = xgb_pipeline.named_steps['regressor'].feature_importances_\n",
    "xgb_feature_names = preprocessor.get_feature_names_out()\n",
    "xgb_feature_importances_df = pd.DataFrame({'Feature': xgb_feature_names, 'Importance': xgb_feature_importances})\n",
    "xgb_feature_importances_df = xgb_feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances of XGBoost\n",
    "print(\"\\nXGBoost Feature Importances (Top 20):\")\n",
    "print(xgb_feature_importances_df[:20])  # Display top 20 features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Regressor\n",
    "lgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbosity=-1))\n",
    "])\n",
    "lgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the validation set with LightGBM Regressor\n",
    "y_val_pred_lgb = lgb_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate regression metrics for validation set with LightGBM Regressor\n",
    "val_mae_lgb = mean_absolute_error(y_val, y_val_pred_lgb)\n",
    "val_mse_lgb = mean_squared_error(y_val, y_val_pred_lgb)\n",
    "val_rmse_lgb = root_mean_squared_error(y_val, y_val_pred_lgb)\n",
    "val_r2_lgb = r2_score(y_val, y_val_pred_lgb)\n",
    "\n",
    "# Display the metrics for LightGBM Regressor\n",
    "print(\"LightGBM Regressor Metrics:\")\n",
    "print(f\"LightGBM Validation MAE: {val_mae_lgb}\")\n",
    "print(f\"LightGBM Validation MSE: {val_mse_lgb}\")\n",
    "print(f\"LightGBM Validation RMSE: {val_rmse_lgb}\")\n",
    "print(f\"LightGBM Validation R²: {val_r2_lgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature importances of LightGBM\n",
    "lgb_feature_importances = lgb_pipeline.named_steps['regressor'].feature_importances_\n",
    "lgb_feature_names = preprocessor.get_feature_names_out()\n",
    "lgb_feature_importances_df = pd.DataFrame({'Feature': lgb_feature_names, 'Importance': lgb_feature_importances})\n",
    "lgb_feature_importances_df = lgb_feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "# Print the feature importances of LightGBM\n",
    "print(\"\\nLightGBM Feature Importances (Top 20):\")\n",
    "print(lgb_feature_importances_df[:20])  # Display top 20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ing metrics to dictionary for each model\n",
    "metrics_dt = {\n",
    "    \"Model\": \"Decision Tree\",\n",
    "    \"MAE\":  val_mae_dt,\n",
    "    \"RMSE\": val_rmse_dt,\n",
    "    \"R2\":   val_r2_dt,\n",
    "}\n",
    "\n",
    "metrics_lgb = {\n",
    "    \"Model\": \"LightGBM\",\n",
    "    \"MAE\":  val_mae_lgb,\n",
    "    \"RMSE\": val_rmse_lgb,\n",
    "    \"R2\":   val_r2_lgb,\n",
    "}\n",
    "\n",
    "metrics_rf = {\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"MAE\":  val_mae_rf,\n",
    "    \"RMSE\": val_rmse_rf,\n",
    "    \"R2\":   val_r2_rf,\n",
    "}\n",
    "\n",
    "metrics_xgb = {\n",
    "    \"Model\": \"XGBoost\",\n",
    "    \"MAE\":  val_mae_xgb,\n",
    "    \"RMSE\": val_rmse_xgb,\n",
    "    \"R2\":   val_r2_xgb,\n",
    "}\n",
    "\n",
    "\n",
    "all_results = [metrics_dt, metrics_rf, metrics_xgb, metrics_lgb]\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(all_results)\n",
    "      .set_index(\"Model\")\n",
    "      .round(4)          # nice, tidy formatting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved for final sanity check\n",
    "dt_default_model = dt_pipeline\n",
    "rf_default_model = rf_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without any parameters tuning, Random Forest shows the most promising results, followed by XGBoost and LightGBM. We will use the 3 models for further hyperparameter tuning. Decision tree without tuning will be the base line model for sanity check.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSE is too difficult to interpret and thus we have dropped it from the analysis. We will be using MAE, RMSE and R-squared. We should look at all the different performance metrics MAE, RMSE and R2 because different performance metric present different information. MAE measure the magnitude of errors but it is less sensitive to outliers. RMSE penalized large errors and helps to identify if you have large outliers. R2 tell us how much of the variation in the target are explainable by our model. However, it cannot tell us if the predictions are bias.**\n",
    "\n",
    "**Our analysis above shows that RMSE and larger than MAE indicating that there are outliers that RMSE amplified. We will not use RMSE as we do not want to penalized the squared error since the outliers are the high end housing market that our property firm would want to served. Our R-squared are consistently high indicating that our tree-based models can explain the variation in the target better. However, the differences between difference R-squared could not help us to explain the metrics further. In conclusion, we would use MAE as our primary metrics as it is also easier to explained to the management of the property firm.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search Strategy**\n",
    "\n",
    "We will use **MAE** as the main metric because it is most easy to be understood by the stakeholder.\n",
    "\n",
    "We have performed fine tuning with a 3 stage parameters fine tuning starting with **Randomized Search** for stage 1 using the widest search space. Then we will use **Halving Randomized Search** based on the search result of stage 1 and finally, we will use **Optuna Search** to finalized the search parameters.\n",
    "\n",
    "However, after several hours of fine tuning, our MAE did improved a few hundred dollars. This is not acceptable as the improvement is marginal compared to the resource we have put in. Thus we will be using randomized search  with ranges around the default as sanity check.\n",
    "\n",
    "For further fine tuning, we will be using optuna with 5 cross validation folder around our searched parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search CV Parameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RansomizedSearchCV parameters with wide search space for Decision Tree Regressor, Random Forest Regressor, XGBoost Regressor, and LightGBM Regressor\n",
    "\n",
    "rf_param_grid = {\n",
    "    'regressor__n_estimators':    [100, 200, 300],       \n",
    "    'regressor__max_depth':       [40, 50, 60],           \n",
    "    'regressor__min_samples_split': [2, 5, 7, 10],\n",
    "    'regressor__min_samples_leaf':  [1, 5, 7, 10],\n",
    "    'regressor__max_features':      ['sqrt', 'log2', 0.5, 1.0]\n",
    "}   \n",
    "\n",
    "xgb_param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 200],      \n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "    'regressor__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'regressor__gamma': [0, 0.1, 0.2],\n",
    "    'regressor__reg_alpha': [0, 0.1, 1],\n",
    "    'regressor__reg_lambda': [0, 0.1, 1]\n",
    "}   \n",
    "\n",
    "lgb_param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [3, 5, 7, 9],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'regressor__num_leaves': [31, 63, 127],\n",
    "    'regressor__min_child_samples': [20, 30, 40],\n",
    "    'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "    'regressor__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'regressor__reg_alpha': [0, 0.1, 1],\n",
    "    'regressor__reg_lambda': [0, 0.1, 1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=rf_param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_iter=5,\n",
    "    cv=3,\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform randomized search for XGBoost Regressor\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=60,  # Number of iterations for random search\n",
    "    scoring='neg_mean_absolute_error',   \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "xgb_random_search.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform randomized search for LightGBM Regressor\n",
    "lgb_random_search = RandomizedSearchCV(\n",
    "    lgb_pipeline,\n",
    "    param_distributions=lgb_param_grid,\n",
    "    n_iter=30,  # Number of iterations for random search\n",
    "    scoring='neg_mean_absolute_error',       \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "lgb_random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and best score for Random Forest Regressor\n",
    "print(\"Best parameters for Random Forest Regressor:\", rf_random_search.best_params_)\n",
    "print(\"Best score for Random Forest Regressor (negative MSE):\", rf_random_search.best_score_)\n",
    "# Print the best parameters and best score for XGBoost Regressor\n",
    "print(\"Best parameters for XGBoost Regressor:\", xgb_random_search.best_params_)\n",
    "print(\"Best score for XGBoost Regressor (negative MSE):\", xgb_random_search.best_score_)\n",
    "# Print the best parameters and best score for LightGBM Regressor\n",
    "print(\"Best parameters for LightGBM Regressor:\", lgb_random_search.best_params_)\n",
    "print(\"Best score for LightGBM Regressor (negative MSE):\", lgb_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The difference between the best model is not that great. Will perform fine tuning and select the best few for final model evaluation test.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tight_distributions(best_params,\n",
    "                             int_frac: float = 0.2,\n",
    "                             float_frac: float = 0.1,\n",
    "                             min_int_step: int = 1):\n",
    "    \"\"\"\n",
    "    Given a dict of best_params_, return a dict of\n",
    "    Optuna Distributions that span ±frac around each value.\n",
    "    \"\"\"\n",
    "    tight_dists = {}\n",
    "    for name, val in best_params.items():\n",
    "        # only handle numeric params\n",
    "        if isinstance(val, int):\n",
    "            # window = max(val * int_frac, min_int_step)\n",
    "            window = max(int(val * int_frac), min_int_step)\n",
    "            low  = max(1, val - window)      # avoid zero or negative\n",
    "            high = val + window\n",
    "            # choose step = min_int_step or window itself\n",
    "            step = min_int_step if min_int_step <= window else window\n",
    "            tight_dists[name] = IntDistribution(low=low, high=high, step=step)\n",
    "\n",
    "        elif isinstance(val, float):\n",
    "            window = val * float_frac\n",
    "            low  = max(0.0, val - window)\n",
    "            high = min(1.0, val + window)    # assuming [0,1] support for fractions\n",
    "            tight_dists[name] = FloatDistribution(low=low, high=high)\n",
    "\n",
    "        else:\n",
    "            # skip non-numeric (e.g. categorical) or handle separately\n",
    "            continue\n",
    "\n",
    "    return tight_dists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Optuna Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. grab your previously-found best params:\n",
    "best_rf_random_search = rf_random_search.best_params_\n",
    "\n",
    "# 2. build the “around-the-best” distributions:\n",
    "rf_param_distributions = make_tight_distributions(best_rf_random_search,\n",
    "                                               int_frac=0.2,     # ±20%\n",
    "                                               float_frac=0.1,   # ±10%\n",
    "                                               min_int_step=1)   # at least step=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Optuna quick search\n",
    "optuna_search = OptunaSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_distributions=rf_param_distributions,\n",
    "    cv=5,             \n",
    "    n_trials=2,      \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# 4. Run the search\n",
    "optuna_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inspect results\n",
    "print(\"Best MAE  =\", optuna_search.best_score_)\n",
    "print(\"Best params:\")\n",
    "for k, v in optuna_search.best_params_.items():\n",
    "    print(f\"  • {k} = {v}\")\n",
    "\n",
    "# 6. Your final model\n",
    "final_rf_model = optuna_search.best_estimator_\n",
    "final_rf_model_params = optuna_search.best_params_\n",
    "final_rf_model_scores = optuna_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final_rf_model_pred = final_rf_model.predict(X_val)\n",
    "\n",
    "# 7. Calculate regression metrics for validation set with Random Forest Regressor\n",
    "final_val_mae_rf = mean_absolute_error(y_val, val_final_rf_model_pred)\n",
    "final_val_mse_rf = mean_squared_error(y_val, val_final_rf_model_pred)\n",
    "final_val_rmse_rf = root_mean_squared_error(y_val, val_final_rf_model_pred)\n",
    "final_val_r2_rf = r2_score(y_val, val_final_rf_model_pred)\n",
    "\n",
    "# 8. Display the metrics for Random Forest Regressor\n",
    "print(\"Random Forest Regressor Metrics:\")\n",
    "print(f\"Random Forest Validation MAE: {final_val_mae_rf}\")\n",
    "print(f\"Random Forest Validation MSE: {final_val_mse_rf}\")\n",
    "print(f\"Random Forest Validation RMSE: {final_val_rmse_rf}\")\n",
    "print(f\"Random Forest Validation R²: {final_val_r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Optuna Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. grab your previously-found best params:\n",
    "best_xgb_random_search_param = xgb_random_search.best_params_\n",
    "\n",
    "# 2. build the “around-the-best” distributions:\n",
    "xgb_param_distributions = make_tight_distributions(best_xgb_random_search_param,\n",
    "                                               int_frac=0.2,     # ±20%\n",
    "                                               float_frac=0.1,   # ±10%\n",
    "                                               min_int_step=1)   # at least step=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Optuna quick search\n",
    "optuna_search = OptunaSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=xgb_param_distributions,\n",
    "    cv=5,             \n",
    "    n_trials=50,      \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# 4. Run the search\n",
    "optuna_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inspect results\n",
    "print(\"Best MAE  =\", optuna_search.best_score_)\n",
    "print(\"Best params:\")\n",
    "for k, v in optuna_search.best_params_.items():\n",
    "    print(f\"  • {k} = {v}\")\n",
    "\n",
    "# 6. Your final model\n",
    "final_xgb_model = optuna_search.best_estimator_\n",
    "final_xgb_model_params = optuna_search.best_params_\n",
    "final_xgb_model_scores = optuna_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final_xgb_model = final_xgb_model.predict(X_val)\n",
    "\n",
    "# 7. Calculate regression metrics for validation set with Random Forest Regressor\n",
    "final_val_mae_xgb = mean_absolute_error(y_val, val_final_xgb_model)\n",
    "final_val_mse_xgb = mean_squared_error(y_val, val_final_xgb_model)\n",
    "final_val_rmse_xgb = root_mean_squared_error(y_val, val_final_xgb_model)\n",
    "final_val_r2_xgb = r2_score(y_val, val_final_xgb_model)\n",
    "\n",
    "# 8. Display the metrics for Random Forest Regressor\n",
    "print(\"Random Forest Regressor Metrics:\")\n",
    "print(f\"Random Forest Validation MAE: {final_val_mae_xgb}\")\n",
    "print(f\"Random Forest Validation MSE: {final_val_mse_xgb}\")\n",
    "print(f\"Random Forest Validation RMSE: {final_val_rmse_xgb}\")\n",
    "print(f\"Random Forest Validation R²: {final_val_r2_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Optuna Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. grab your previously-found best params:\n",
    "best_lgb_random_search_param = lgb_random_search.best_params_\n",
    "\n",
    "# 2. build the “around-the-best” distributions:\n",
    "lgb_param_distributions = make_tight_distributions(best_lgb_random_search_param,\n",
    "                                               int_frac=0.2,     # ±20%\n",
    "                                               float_frac=0.1,   # ±10%\n",
    "                                               min_int_step=1)   # at least step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_param_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Optuna quick search\n",
    "optuna_search = OptunaSearchCV(\n",
    "    estimator=lgb_pipeline,\n",
    "    param_distributions=lgb_param_distributions,\n",
    "    cv=5,             \n",
    "    n_trials=30,      \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# 4. Run the search\n",
    "optuna_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inspect results\n",
    "print(\"Best MAE  =\", optuna_search.best_score_)\n",
    "print(\"Best params:\")\n",
    "for k, v in optuna_search.best_params_.items():\n",
    "    print(f\"  • {k} = {v!r}\")\n",
    "\n",
    "# 6. Your final model\n",
    "final_lgb_model = optuna_search.best_estimator_\n",
    "final_lgb_model_params = optuna_search.best_params_\n",
    "final_lgb_model_scores = optuna_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final_lgb_model = final_lgb_model.predict(X_val)\n",
    "\n",
    "# 7. Calculate regression metrics for validation set with Random Forest Regressor\n",
    "final_val_mae_lgb = mean_absolute_error(y_val, val_final_lgb_model)\n",
    "final_val_mse_lgb = mean_squared_error(y_val, val_final_lgb_model)\n",
    "final_val_rmse_lgb = root_mean_squared_error(y_val, val_final_lgb_model)\n",
    "final_val_r2_lgb = r2_score(y_val, val_final_lgb_model)\n",
    "\n",
    "# 8. Display the metrics for Random Forest Regressor\n",
    "print(\"Random Forest Regressor Metrics:\")\n",
    "print(f\"Random Forest Validation MAE: {final_val_mae_lgb}\")\n",
    "print(f\"Random Forest Validation MSE: {final_val_mse_lgb}\")\n",
    "print(f\"Random Forest Validation RMSE: {final_val_rmse_lgb}\")\n",
    "print(f\"Random Forest Validation R²: {final_val_r2_lgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Fine Tuned Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and best score for Random Forest Regressor\n",
    "print(\"Best parameters for Random Forest Regressor:\", final_rf_model_params)\n",
    "print(\"Best score for Random Forest Regressor (negative MSE):\", final_rf_model_scores)\n",
    "# Print the best parameters and best score for XGBoost Regressor\n",
    "print(\"Best parameters for XGBoost Regressor:\", final_xgb_model_params)\n",
    "print(\"Best score for XGBoost Regressor (negative MSE):\", final_xgb_model_scores)\n",
    "# Print the best parameters and best score for LightGBM Regressor\n",
    "print(\"Best parameters for LightGBM Regressor:\", final_lgb_model_params)\n",
    "print(\"Best score for LightGBM Regressor (negative MSE):\", final_lgb_model_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Fine Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we apply the default decision tree as baseline for sanity check.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set with the default Decision Tree Regressor\n",
    "y_val_pred_dt_default = dt_default_model.predict(X_val)\n",
    "# Calculate regression metrics for validation set with the default Decision Tree Regressor\n",
    "val_mae_dt_default = mean_absolute_error(y_val, y_val_pred_dt_default)\n",
    "val_rmse_dt_default = root_mean_squared_error(y_val, y_val_pred_dt_default)\n",
    "val_r2_dt_default = r2_score(y_val, y_val_pred_dt_default)\n",
    "\n",
    "# Display the metrics for the default Decision Tree Regressor\n",
    "print(\"Default Decision Tree Regressor Metrics:\")\n",
    "print(f\"Default Decision Tree Validation MAE: {val_mae_dt_default}\")\n",
    "print(f\"Default Decision Tree Validation RMSE: {val_rmse_dt_default}\")\n",
    "print(f\"Default Decision Tree Validation R²: {val_r2_dt_default}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we try Random Forest with default settings as it has a good score before fine tuning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the default Random Forest Regressor\n",
    "y_val_pred_rf_default = rf_default_model.predict(X_val)\n",
    "# Calculate regression metrics for test set with the default Random Forest Regressor\n",
    "val_mae_rf_default = mean_absolute_error(y_val, y_val_pred_rf_default)\n",
    "val_rmse_rf_default = root_mean_squared_error(y_val, y_val_pred_rf_default)\n",
    "val_r2_rf_default = r2_score(y_val, y_val_pred_rf_default)\n",
    "\n",
    "# Display the metrics for the default Random Forest Regressor\n",
    "print(\"Default Random Forest Regressor Metrics:\")\n",
    "print(f\"Default Random Forest Validation MAE: {val_mae_rf_default}\")\n",
    "print(f\"Default Random Forest Validation RMSE: {val_rmse_rf_default}\")\n",
    "print(f\"Default Random Forest Validation R²: {val_r2_rf_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the validation set with the best Random Forest Regressor\n",
    "y_val_pred_rf_best = final_rf_model.predict(X_val)\n",
    "# Calculate regression metrics for validation set with the best Random Forest Regressor\n",
    "val_mae_rf_best = mean_absolute_error(y_val, y_val_pred_rf_best)\n",
    "val_rmse_rf_best = root_mean_squared_error(y_val, y_val_pred_rf_best)\n",
    "val_r2_rf_best = r2_score(y_val, y_val_pred_rf_best)    \n",
    "\n",
    "# Display the metrics for the best Random Forest Regressor\n",
    "print(\"Best Random Forest Regressor Metrics:\")\n",
    "print(f\"Best Random Forest Validation MAE: {val_mae_rf_best}\")\n",
    "print(f\"Best Random Forest Validation RMSE: {val_rmse_rf_best}\")\n",
    "print(f\"Best Random Forest Validation R²: {val_r2_rf_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the validation set with the best xgboost Regressor\n",
    "y_val_pred_xgb_best = final_xgb_model.predict(X_val)\n",
    "# Calculate regression metrics for validation set with the best XGBoost Regressor\n",
    "val_mae_xgb_best = mean_absolute_error(y_val, y_val_pred_xgb_best)\n",
    "val_rmse_xgb_best = root_mean_squared_error(y_val, y_val_pred_xgb_best)\n",
    "val_r2_xgb_best = r2_score(y_val, y_val_pred_xgb_best)  \n",
    "# Display the metrics for the best XGBoost Regressor\n",
    "print(\"Best XGBoost Regressor Metrics:\")\n",
    "print(f\"Best XGBoost Validation MAE: {val_mae_xgb_best}\")       \n",
    "print(f\"Best XGBoost Validation RMSE: {val_rmse_xgb_best}\")\n",
    "print(f\"Best XGBoost Validation R²: {val_r2_xgb_best}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the validation set with the best lightgbm Regressor\n",
    "y_val_pred_lgb_best = final_lgb_model.predict(X_val)\n",
    "# Calculate regression metrics for validation set with the best LightGBM Regressor\n",
    "val_mae_lgb_best = mean_absolute_error(y_val, y_val_pred_lgb_best)\n",
    "val_rmse_lgb_best = root_mean_squared_error(y_val, y_val_pred_lgb_best)\n",
    "val_r2_lgb_best = r2_score(y_val, y_val_pred_lgb_best)\n",
    "# Display the metrics for the best LightGBM Regressor\n",
    "print(\"Best LightGBM Regressor Metrics:\")\n",
    "print(f\"Best LightGBM Validation MAE: {val_mae_lgb_best}\")\n",
    "print(f\"Best LightGBM Validation RMSE: {val_rmse_lgb_best}\")\n",
    "print(f\"Best LightGBM Validation R²: {val_r2_lgb_best}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ing metrics to dictionary for each model\n",
    "metrics_rf_default = {\n",
    "    \"Model\": \"Random Forest (def)\",\n",
    "    \"MAE\":  val_mae_rf_default,\n",
    "    \"RMSE\": val_rmse_rf_default,\n",
    "    \"R2\":   val_r2_rf_default,\n",
    "}\n",
    "\n",
    "metrics_rf = {\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"MAE\":  val_mae_rf_best,\n",
    "    \"RMSE\": val_rmse_rf_best,\n",
    "    \"R2\":   val_r2_rf_best,\n",
    "}\n",
    "\n",
    "metrics_xgb = {\n",
    "    \"Model\": \"XGBoost\",\n",
    "    \"MAE\":  val_mae_xgb_best,\n",
    "    \"RMSE\": val_rmse_xgb_best,\n",
    "    \"R2\":   val_r2_xgb_best,\n",
    "}\n",
    "\n",
    "metrics_lgb = {\n",
    "    \"Model\": \"LightGBM\",\n",
    "    \"MAE\":  val_mae_lgb_best,\n",
    "    \"RMSE\": val_rmse_lgb_best,\n",
    "    \"R2\":   val_r2_lgb_best,\n",
    "}\n",
    "\n",
    "all_results = [metrics_rf_default, metrics_rf, metrics_xgb, metrics_lgb]\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(all_results)\n",
    "      .set_index(\"Model\")\n",
    "      .round(4)          # nice, tidy formatting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our best model is XGBoost with MAE of $23,338 margin or errors.**\n",
    "\n",
    "**All MAE are very close. In our experience, model with the best validation score may not do well in the test. Therefore, we will apply all the 4 models into the test set. We will deploy model with the best score.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the default Random Forest Regressor\n",
    "y_test_pred_rf_default = rf_default_model.predict(X_test)\n",
    "# Calculate regression metrics for test set with the default Random Forest Regressor\n",
    "test_mae_rf_default = mean_absolute_error(y_test, y_test_pred_rf_default)\n",
    "test_rmse_rf_default = root_mean_squared_error(y_test, y_test_pred_rf_default)\n",
    "test_r2_rf_default = r2_score(y_test, y_test_pred_rf_default)\n",
    "\n",
    "# Display the metrics for the default Random Forest Regressor\n",
    "print(\"Default Random Forest Regressor Metrics:\")\n",
    "print(f\"Default Random Forest Test MAE: {test_mae_rf_default}\")\n",
    "print(f\"Default Random Forest Test RMSE: {test_rmse_rf_default}\")\n",
    "print(f\"Default Random Forest Test R²: {test_r2_rf_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best Random Forest Regressor\n",
    "y_test_pred_rf_best = final_rf_model.predict(X_test)\n",
    "# Calculate regression metrics for test set with the best Random Forest Regressor\n",
    "test_mae_rf_best = mean_absolute_error(y_test, y_test_pred_rf_best)\n",
    "test_rmse_rf_best = root_mean_squared_error(y_test, y_test_pred_rf_best)\n",
    "test_r2_rf_best = r2_score(y_test, y_test_pred_rf_best)\n",
    "\n",
    "# Display the metrics for the best Random Forest Regressor\n",
    "print(\"Best Random Forest Regressor Metrics:\")\n",
    "print(f\"Best Random Forest Test MAE: {test_mae_rf_best}\")\n",
    "print(f\"Best Random Forest Test RMSE: {test_rmse_rf_best}\")\n",
    "print(f\"Best Random Forest Test R²: {test_r2_rf_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best XGBoost Regressor\n",
    "y_test_pred_xgb_best = final_xgb_model.predict(X_test)\n",
    "# Calculate regression metrics for test set with the best XGBoost Regressor\n",
    "test_mae_xgb_best = mean_absolute_error(y_test, y_test_pred_xgb_best)\n",
    "test_rmse_xgb_best = root_mean_squared_error(y_test, y_test_pred_xgb_best)\n",
    "test_r2_xgb_best = r2_score(y_test, y_test_pred_xgb_best)\n",
    "\n",
    "# Display the metrics for the best XGBoost Regressor\n",
    "print(\"Best XGBoost Regressor Metrics:\")\n",
    "print(f\"Best XGBoost Regressor Test MAE: {test_mae_xgb_best}\")\n",
    "print(f\"Best XGBoost Regressor Test RMSE: {test_rmse_xgb_best}\")\n",
    "print(f\"Best XGBoost Regressor Test R²: {test_r2_xgb_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set with the best LightGBM Regressor\n",
    "y_test_pred_lgb_best = final_lgb_model.predict(X_test)\n",
    "# Calculate regression metrics for test set with the best LightGBM Regressor\n",
    "test_mae_lgb_best = mean_absolute_error(y_test, y_test_pred_lgb_best)\n",
    "test_rmse_lgb_best = root_mean_squared_error(y_test, y_test_pred_lgb_best)\n",
    "test_r2_lgb_best = r2_score(y_test, y_test_pred_lgb_best)\n",
    "\n",
    "# Display the metrics for the best LightGBM Regressor\n",
    "print(\"Best LightGBM Regressor Metrics:\")\n",
    "print(f\"Best LightGBM Regressor Test MAE: {test_mae_lgb_best}\")\n",
    "print(f\"Best LightGBM Regressor Test RMSE: {test_rmse_lgb_best}\")\n",
    "print(f\"Best LightGBM Regressor Test R²: {test_r2_lgb_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best model is still XGBoost. MAE result is similar to validation test. This confirms that our test set is representative of the validation datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model from Optuna Search\n",
    "best_model = final_xgb_model\n",
    "\n",
    "# Predict on the test set with Ridge Regression\n",
    "y_test_pred_best_model = best_model.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics for the test set for Ridge\n",
    "test_mae_best = mean_absolute_error(y_test, y_test_pred_best_model)\n",
    "test_rmse_best = root_mean_squared_error(y_test, y_test_pred_best_model)\n",
    "test_r2_best = r2_score(y_test, y_test_pred_best_model)\n",
    "\n",
    "print(\"Best Ridge Regression Model, Final Test Metrics:\")\n",
    "print(f\"Final Test MAE: {test_mae_best}\")\n",
    "print(f\"Final Test RMSE: {test_rmse_best}\")\n",
    "print(f\"Final Test R²: {test_r2_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
